{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Visualizing Training Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Load Datasets\n",
    "BASE_PATH='/kaggle/input/characters-minimized/DATA/'\n",
    "train_dataset=pd.read_csv(os.path.join(BASE_PATH,'train.csv'))\n",
    "test_dataset=pd.read_csv(os.path.join(BASE_PATH,'test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>train_31_29572</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>train_31_29713</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>train_31_29949</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>train_31_29651</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>train_31_29745</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        filename  class\n0           0  train_31_29572      1\n1           1  train_31_29713      1\n2           2  train_31_29949      1\n3           3  train_31_29651      1\n4           4  train_31_29745      1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>train_31_30009</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>train_31_29999</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>train_31_30029</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>train_31_30031</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>train_31_30019</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        filename  class\n0           0  train_31_30009      1\n1           1  train_31_29999      1\n2           2  train_31_30029      1\n3           3  train_31_30031      1\n4           4  train_31_30019      1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2304x2304 with 15 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pyplot as plt\n",
    "fig=plt.figure(figsize=(32, 32))\n",
    "columns = 3 \n",
    "rows = 5 \n",
    "for i in range(1,rows*columns+1):\n",
    "    IMG_PATH=BASE_PATH+'train_images/'\n",
    "    img=Image.open(os.path.join(IMG_PATH,train_dataset.iloc[i][1]+'.png'))\n",
    "    fig.add_subplot(rows, columns, i)\n",
    "    plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ****Importing and Installing Libraries****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: efficientnet-pytorch in /opt/conda/lib/python3.6/site-packages (0.6.3)\nRequirement already satisfied: torch in /opt/conda/lib/python3.6/site-packages (from efficientnet-pytorch) (1.4.0)\nRequirement already satisfied: torchsummary in /opt/conda/lib/python3.6/site-packages (1.5.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch\n",
    "!pip install torchsummary\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import PIL\n",
    "import sys\n",
    "import torch\n",
    "from time import time\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data-Loader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Dataset(data.Dataset):\n",
    "    def __init__(self,csv_path,images_path,transform=None):\n",
    "        self.train_set=pd.read_csv(csv_path) \n",
    "        self.train_path=images_path\n",
    "        self.transform=transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.train_set)\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        file_name=self.train_set.iloc[idx][1]+'.png'\n",
    "        label=self.train_set.iloc[idx][2]\n",
    "        img=Image.open(os.path.join(self.train_path,file_name)) \n",
    "        if self.transform is not None:\n",
    "            img=self.transform(img)\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Defining Transforms and Parameters for Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#Parameters\n",
    "params = {'batch_size': 16,\n",
    "          'shuffle': True\n",
    "         }\n",
    "epochs = 18\n",
    "learning_rate=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([transforms.Resize((224,224)),transforms.RandomApply([\n",
    "        torchvision.transforms.RandomRotation(10),\n",
    "        transforms.RandomHorizontalFlip()],0.7),\n",
    "\t\ttransforms.ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "training_set=Dataset(os.path.join(BASE_PATH,'train.csv'),os.path.join(BASE_PATH,'train_images/'),transform=transform_train)\n",
    "training_generator=data.DataLoader(training_set,**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Importing the Model (Efficient Net)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n"
     ]
    }
   ],
   "source": [
    "model = EfficientNet.from_pretrained('efficientnet-b0', num_classes=62)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n  (_conv_stem): Conv2dStaticSamePadding(\n    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n  )\n  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_blocks): ModuleList(\n    (0): MBConvBlock(\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        32, 8, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        8, 32, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (1): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        96, 4, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        4, 96, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (2): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (3): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (4): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (5): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (6): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (7): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (8): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (9): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (10): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (11): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (12): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (13): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (14): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (15): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n  )\n  (_conv_head): Conv2dStaticSamePadding(\n    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (static_padding): Identity()\n  )\n  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n  (_dropout): Dropout(p=0.2, inplace=False)\n  (_fc): Linear(in_features=1280, out_features=62, bias=True)\n  (_swish): MemoryEfficientSwish()\n)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n         ZeroPad2d-1          [-1, 3, 513, 513]               0\nConv2dStaticSamePadding-2         [-1, 32, 256, 256]             864\n       BatchNorm2d-3         [-1, 32, 256, 256]              64\nMemoryEfficientSwish-4         [-1, 32, 256, 256]               0\n         ZeroPad2d-5         [-1, 32, 258, 258]               0\nConv2dStaticSamePadding-6         [-1, 32, 256, 256]             288\n       BatchNorm2d-7         [-1, 32, 256, 256]              64\nMemoryEfficientSwish-8         [-1, 32, 256, 256]               0\n          Identity-9             [-1, 32, 1, 1]               0\nConv2dStaticSamePadding-10              [-1, 8, 1, 1]             264\nMemoryEfficientSwish-11              [-1, 8, 1, 1]               0\n         Identity-12              [-1, 8, 1, 1]               0\nConv2dStaticSamePadding-13             [-1, 32, 1, 1]             288\n         Identity-14         [-1, 32, 256, 256]               0\nConv2dStaticSamePadding-15         [-1, 16, 256, 256]             512\n      BatchNorm2d-16         [-1, 16, 256, 256]              32\n      MBConvBlock-17         [-1, 16, 256, 256]               0\n         Identity-18         [-1, 16, 256, 256]               0\nConv2dStaticSamePadding-19         [-1, 96, 256, 256]           1,536\n      BatchNorm2d-20         [-1, 96, 256, 256]             192\nMemoryEfficientSwish-21         [-1, 96, 256, 256]               0\n        ZeroPad2d-22         [-1, 96, 257, 257]               0\nConv2dStaticSamePadding-23         [-1, 96, 128, 128]             864\n      BatchNorm2d-24         [-1, 96, 128, 128]             192\nMemoryEfficientSwish-25         [-1, 96, 128, 128]               0\n         Identity-26             [-1, 96, 1, 1]               0\nConv2dStaticSamePadding-27              [-1, 4, 1, 1]             388\nMemoryEfficientSwish-28              [-1, 4, 1, 1]               0\n         Identity-29              [-1, 4, 1, 1]               0\nConv2dStaticSamePadding-30             [-1, 96, 1, 1]             480\n         Identity-31         [-1, 96, 128, 128]               0\nConv2dStaticSamePadding-32         [-1, 24, 128, 128]           2,304\n      BatchNorm2d-33         [-1, 24, 128, 128]              48\n      MBConvBlock-34         [-1, 24, 128, 128]               0\n         Identity-35         [-1, 24, 128, 128]               0\nConv2dStaticSamePadding-36        [-1, 144, 128, 128]           3,456\n      BatchNorm2d-37        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-38        [-1, 144, 128, 128]               0\n        ZeroPad2d-39        [-1, 144, 130, 130]               0\nConv2dStaticSamePadding-40        [-1, 144, 128, 128]           1,296\n      BatchNorm2d-41        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-42        [-1, 144, 128, 128]               0\n         Identity-43            [-1, 144, 1, 1]               0\nConv2dStaticSamePadding-44              [-1, 6, 1, 1]             870\nMemoryEfficientSwish-45              [-1, 6, 1, 1]               0\n         Identity-46              [-1, 6, 1, 1]               0\nConv2dStaticSamePadding-47            [-1, 144, 1, 1]           1,008\n         Identity-48        [-1, 144, 128, 128]               0\nConv2dStaticSamePadding-49         [-1, 24, 128, 128]           3,456\n      BatchNorm2d-50         [-1, 24, 128, 128]              48\n      MBConvBlock-51         [-1, 24, 128, 128]               0\n         Identity-52         [-1, 24, 128, 128]               0\nConv2dStaticSamePadding-53        [-1, 144, 128, 128]           3,456\n      BatchNorm2d-54        [-1, 144, 128, 128]             288\nMemoryEfficientSwish-55        [-1, 144, 128, 128]               0\n        ZeroPad2d-56        [-1, 144, 131, 131]               0\nConv2dStaticSamePadding-57          [-1, 144, 64, 64]           3,600\n      BatchNorm2d-58          [-1, 144, 64, 64]             288\nMemoryEfficientSwish-59          [-1, 144, 64, 64]               0\n         Identity-60            [-1, 144, 1, 1]               0\nConv2dStaticSamePadding-61              [-1, 6, 1, 1]             870\nMemoryEfficientSwish-62              [-1, 6, 1, 1]               0\n         Identity-63              [-1, 6, 1, 1]               0\nConv2dStaticSamePadding-64            [-1, 144, 1, 1]           1,008\n         Identity-65          [-1, 144, 64, 64]               0\nConv2dStaticSamePadding-66           [-1, 40, 64, 64]           5,760\n      BatchNorm2d-67           [-1, 40, 64, 64]              80\n      MBConvBlock-68           [-1, 40, 64, 64]               0\n         Identity-69           [-1, 40, 64, 64]               0\nConv2dStaticSamePadding-70          [-1, 240, 64, 64]           9,600\n      BatchNorm2d-71          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-72          [-1, 240, 64, 64]               0\n        ZeroPad2d-73          [-1, 240, 68, 68]               0\nConv2dStaticSamePadding-74          [-1, 240, 64, 64]           6,000\n      BatchNorm2d-75          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-76          [-1, 240, 64, 64]               0\n         Identity-77            [-1, 240, 1, 1]               0\nConv2dStaticSamePadding-78             [-1, 10, 1, 1]           2,410\nMemoryEfficientSwish-79             [-1, 10, 1, 1]               0\n         Identity-80             [-1, 10, 1, 1]               0\nConv2dStaticSamePadding-81            [-1, 240, 1, 1]           2,640\n         Identity-82          [-1, 240, 64, 64]               0\nConv2dStaticSamePadding-83           [-1, 40, 64, 64]           9,600\n      BatchNorm2d-84           [-1, 40, 64, 64]              80\n      MBConvBlock-85           [-1, 40, 64, 64]               0\n         Identity-86           [-1, 40, 64, 64]               0\nConv2dStaticSamePadding-87          [-1, 240, 64, 64]           9,600\n      BatchNorm2d-88          [-1, 240, 64, 64]             480\nMemoryEfficientSwish-89          [-1, 240, 64, 64]               0\n        ZeroPad2d-90          [-1, 240, 65, 65]               0\nConv2dStaticSamePadding-91          [-1, 240, 32, 32]           2,160\n      BatchNorm2d-92          [-1, 240, 32, 32]             480\nMemoryEfficientSwish-93          [-1, 240, 32, 32]               0\n         Identity-94            [-1, 240, 1, 1]               0\nConv2dStaticSamePadding-95             [-1, 10, 1, 1]           2,410\nMemoryEfficientSwish-96             [-1, 10, 1, 1]               0\n         Identity-97             [-1, 10, 1, 1]               0\nConv2dStaticSamePadding-98            [-1, 240, 1, 1]           2,640\n         Identity-99          [-1, 240, 32, 32]               0\nConv2dStaticSamePadding-100           [-1, 80, 32, 32]          19,200\n     BatchNorm2d-101           [-1, 80, 32, 32]             160\n     MBConvBlock-102           [-1, 80, 32, 32]               0\n        Identity-103           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-104          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-105          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-106          [-1, 480, 32, 32]               0\n       ZeroPad2d-107          [-1, 480, 34, 34]               0\nConv2dStaticSamePadding-108          [-1, 480, 32, 32]           4,320\n     BatchNorm2d-109          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-110          [-1, 480, 32, 32]               0\n        Identity-111            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-112             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-113             [-1, 20, 1, 1]               0\n        Identity-114             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-115            [-1, 480, 1, 1]          10,080\n        Identity-116          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-117           [-1, 80, 32, 32]          38,400\n     BatchNorm2d-118           [-1, 80, 32, 32]             160\n     MBConvBlock-119           [-1, 80, 32, 32]               0\n        Identity-120           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-121          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-122          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-123          [-1, 480, 32, 32]               0\n       ZeroPad2d-124          [-1, 480, 34, 34]               0\nConv2dStaticSamePadding-125          [-1, 480, 32, 32]           4,320\n     BatchNorm2d-126          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-127          [-1, 480, 32, 32]               0\n        Identity-128            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-129             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-130             [-1, 20, 1, 1]               0\n        Identity-131             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-132            [-1, 480, 1, 1]          10,080\n        Identity-133          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-134           [-1, 80, 32, 32]          38,400\n     BatchNorm2d-135           [-1, 80, 32, 32]             160\n     MBConvBlock-136           [-1, 80, 32, 32]               0\n        Identity-137           [-1, 80, 32, 32]               0\nConv2dStaticSamePadding-138          [-1, 480, 32, 32]          38,400\n     BatchNorm2d-139          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-140          [-1, 480, 32, 32]               0\n       ZeroPad2d-141          [-1, 480, 36, 36]               0\nConv2dStaticSamePadding-142          [-1, 480, 32, 32]          12,000\n     BatchNorm2d-143          [-1, 480, 32, 32]             960\nMemoryEfficientSwish-144          [-1, 480, 32, 32]               0\n        Identity-145            [-1, 480, 1, 1]               0\nConv2dStaticSamePadding-146             [-1, 20, 1, 1]           9,620\nMemoryEfficientSwish-147             [-1, 20, 1, 1]               0\n        Identity-148             [-1, 20, 1, 1]               0\nConv2dStaticSamePadding-149            [-1, 480, 1, 1]          10,080\n        Identity-150          [-1, 480, 32, 32]               0\nConv2dStaticSamePadding-151          [-1, 112, 32, 32]          53,760\n     BatchNorm2d-152          [-1, 112, 32, 32]             224\n     MBConvBlock-153          [-1, 112, 32, 32]               0\n        Identity-154          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-155          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-156          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-157          [-1, 672, 32, 32]               0\n       ZeroPad2d-158          [-1, 672, 36, 36]               0\nConv2dStaticSamePadding-159          [-1, 672, 32, 32]          16,800\n     BatchNorm2d-160          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-161          [-1, 672, 32, 32]               0\n        Identity-162            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-163             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-164             [-1, 28, 1, 1]               0\n        Identity-165             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-166            [-1, 672, 1, 1]          19,488\n        Identity-167          [-1, 672, 32, 32]               0\nConv2dStaticSamePadding-168          [-1, 112, 32, 32]          75,264\n     BatchNorm2d-169          [-1, 112, 32, 32]             224\n     MBConvBlock-170          [-1, 112, 32, 32]               0\n        Identity-171          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-172          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-173          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-174          [-1, 672, 32, 32]               0\n       ZeroPad2d-175          [-1, 672, 36, 36]               0\nConv2dStaticSamePadding-176          [-1, 672, 32, 32]          16,800\n     BatchNorm2d-177          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-178          [-1, 672, 32, 32]               0\n        Identity-179            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-180             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-181             [-1, 28, 1, 1]               0\n        Identity-182             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-183            [-1, 672, 1, 1]          19,488\n        Identity-184          [-1, 672, 32, 32]               0\nConv2dStaticSamePadding-185          [-1, 112, 32, 32]          75,264\n     BatchNorm2d-186          [-1, 112, 32, 32]             224\n     MBConvBlock-187          [-1, 112, 32, 32]               0\n        Identity-188          [-1, 112, 32, 32]               0\nConv2dStaticSamePadding-189          [-1, 672, 32, 32]          75,264\n     BatchNorm2d-190          [-1, 672, 32, 32]           1,344\nMemoryEfficientSwish-191          [-1, 672, 32, 32]               0\n       ZeroPad2d-192          [-1, 672, 35, 35]               0\nConv2dStaticSamePadding-193          [-1, 672, 16, 16]          16,800\n     BatchNorm2d-194          [-1, 672, 16, 16]           1,344\nMemoryEfficientSwish-195          [-1, 672, 16, 16]               0\n        Identity-196            [-1, 672, 1, 1]               0\nConv2dStaticSamePadding-197             [-1, 28, 1, 1]          18,844\nMemoryEfficientSwish-198             [-1, 28, 1, 1]               0\n        Identity-199             [-1, 28, 1, 1]               0\nConv2dStaticSamePadding-200            [-1, 672, 1, 1]          19,488\n        Identity-201          [-1, 672, 16, 16]               0\nConv2dStaticSamePadding-202          [-1, 192, 16, 16]         129,024\n     BatchNorm2d-203          [-1, 192, 16, 16]             384\n     MBConvBlock-204          [-1, 192, 16, 16]               0\n        Identity-205          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-206         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-207         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-208         [-1, 1152, 16, 16]               0\n       ZeroPad2d-209         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-210         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-211         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-212         [-1, 1152, 16, 16]               0\n        Identity-213           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-214             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-215             [-1, 48, 1, 1]               0\n        Identity-216             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-217           [-1, 1152, 1, 1]          56,448\n        Identity-218         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-219          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-220          [-1, 192, 16, 16]             384\n     MBConvBlock-221          [-1, 192, 16, 16]               0\n        Identity-222          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-223         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-224         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-225         [-1, 1152, 16, 16]               0\n       ZeroPad2d-226         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-227         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-228         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-229         [-1, 1152, 16, 16]               0\n        Identity-230           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-231             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-232             [-1, 48, 1, 1]               0\n        Identity-233             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-234           [-1, 1152, 1, 1]          56,448\n        Identity-235         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-236          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-237          [-1, 192, 16, 16]             384\n     MBConvBlock-238          [-1, 192, 16, 16]               0\n        Identity-239          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-240         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-241         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-242         [-1, 1152, 16, 16]               0\n       ZeroPad2d-243         [-1, 1152, 20, 20]               0\nConv2dStaticSamePadding-244         [-1, 1152, 16, 16]          28,800\n     BatchNorm2d-245         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-246         [-1, 1152, 16, 16]               0\n        Identity-247           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-248             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-249             [-1, 48, 1, 1]               0\n        Identity-250             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-251           [-1, 1152, 1, 1]          56,448\n        Identity-252         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-253          [-1, 192, 16, 16]         221,184\n     BatchNorm2d-254          [-1, 192, 16, 16]             384\n     MBConvBlock-255          [-1, 192, 16, 16]               0\n        Identity-256          [-1, 192, 16, 16]               0\nConv2dStaticSamePadding-257         [-1, 1152, 16, 16]         221,184\n     BatchNorm2d-258         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-259         [-1, 1152, 16, 16]               0\n       ZeroPad2d-260         [-1, 1152, 18, 18]               0\nConv2dStaticSamePadding-261         [-1, 1152, 16, 16]          10,368\n     BatchNorm2d-262         [-1, 1152, 16, 16]           2,304\nMemoryEfficientSwish-263         [-1, 1152, 16, 16]               0\n        Identity-264           [-1, 1152, 1, 1]               0\nConv2dStaticSamePadding-265             [-1, 48, 1, 1]          55,344\nMemoryEfficientSwish-266             [-1, 48, 1, 1]               0\n        Identity-267             [-1, 48, 1, 1]               0\nConv2dStaticSamePadding-268           [-1, 1152, 1, 1]          56,448\n        Identity-269         [-1, 1152, 16, 16]               0\nConv2dStaticSamePadding-270          [-1, 320, 16, 16]         368,640\n     BatchNorm2d-271          [-1, 320, 16, 16]             640\n     MBConvBlock-272          [-1, 320, 16, 16]               0\n        Identity-273          [-1, 320, 16, 16]               0\nConv2dStaticSamePadding-274         [-1, 1280, 16, 16]         409,600\n     BatchNorm2d-275         [-1, 1280, 16, 16]           2,560\nMemoryEfficientSwish-276         [-1, 1280, 16, 16]               0\nAdaptiveAvgPool2d-277           [-1, 1280, 1, 1]               0\n         Dropout-278                 [-1, 1280]               0\n          Linear-279                   [-1, 62]          79,422\n================================================================\nTotal params: 4,086,970\nTrainable params: 4,086,970\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 3.00\nForward/backward pass size (MB): 1091.37\nParams size (MB): 15.59\nEstimated Total Size (MB): 1109.96\n----------------------------------------------------------------\nNone\n"
     ]
    }
   ],
   "source": [
    "print(summary(model, input_size=(3, 512, 512)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PATH_SAVE='./Weights/'\n",
    "if(not os.path.exists(PATH_SAVE)):\n",
    "    os.mkdir(PATH_SAVE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr_decay=0.99\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Training The Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "eye = torch.eye(62).to(device)\n",
    "classes=[i for i in range(62)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "history_accuracy=[]\n",
    "history_loss=[]\n",
    "epochs = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1  Batch :  50  Loss :   3.439736928939819  Accuracy :  0.17 Time  0.09 s\nEpoch :  1  Batch :  100  Loss :   2.6924143266677856  Accuracy :  0.310625 Time  0.1 s\nEpoch :  1  Batch :  150  Loss :   2.2511788670221966  Accuracy :  0.39666666666666667 Time  0.09 s\nEpoch :  1  Batch :  200  Loss :   1.9686623007059096  Accuracy :  0.45625 Time  0.09 s\nEpoch :  1  Batch :  250  Loss :   1.798134348154068  Accuracy :  0.4885 Time  0.09 s\nEpoch :  1  Batch :  300  Loss :   1.650209134320418  Accuracy :  0.523125 Time  0.11 s\nEpoch :  1  Batch :  350  Loss :   1.5277196447338377  Accuracy :  0.5519642857142857 Time  0.09 s\nEpoch :  1  Batch :  400  Loss :   1.4330515195429325  Accuracy :  0.57421875 Time  0.09 s\nEpoch :  1  Batch :  450  Loss :   1.356212518480089  Accuracy :  0.5919444444444445 Time  0.09 s\nEpoch :  1  Batch :  500  Loss :   1.2948580125570297  Accuracy :  0.6055 Time  0.09 s\nEpoch :  1  Batch :  550  Loss :   1.2393380120938475  Accuracy :  0.6188636363636364 Time  0.09 s\nEpoch :  1  Batch :  600  Loss :   1.1898792455842098  Accuracy :  0.631875 Time  0.09 s\nEpoch :  1  Batch :  650  Loss :   1.1455598861437577  Accuracy :  0.6418269230769231 Time  0.1 s\nEpoch :  1  Batch :  700  Loss :   1.1039347196051053  Accuracy :  0.6522321428571428 Time  0.11 s\nEpoch :  1  Batch :  750  Loss :   1.074763732433319  Accuracy :  0.6598333333333334 Time  0.09 s\nEpoch :  1  Batch :  800  Loss :   1.0481836815923453  Accuracy :  0.66640625 Time  0.09 s\nEpoch :  1  Batch :  850  Loss :   1.0152114308581632  Accuracy :  0.6739705882352941 Time  0.09 s\nEpoch :  1  Batch :  900  Loss :   0.988527326517635  Accuracy :  0.6810416666666667 Time  0.09 s\nEpoch :  1  Batch :  950  Loss :   0.9646338544550694  Accuracy :  0.6868421052631579 Time  0.09 s\nEpoch :  1  Batch :  1000  Loss :   0.9429661025106907  Accuracy :  0.6925 Time  0.1 s\nEpoch :  1  Batch :  1050  Loss :   0.9238418875847544  Accuracy :  0.6973809523809524 Time  0.09 s\nEpoch :  1  Batch :  1100  Loss :   0.9058345116539435  Accuracy :  0.7016477272727273 Time  0.09 s\nEpoch :  1  Batch :  1150  Loss :   0.8887494939306508  Accuracy :  0.7059782608695652 Time  0.09 s\nEpoch :  1  Batch :  1200  Loss :   0.8724748019377391  Accuracy :  0.71046875 Time  0.09 s\nEpoch :  1  Batch :  1250  Loss :   0.8591813815236091  Accuracy :  0.71385 Time  0.09 s\nEpoch :  1  Batch :  1300  Loss :   0.8443738578145321  Accuracy :  0.7177884615384615 Time  0.09 s\nEpoch :  1  Batch :  1350  Loss :   0.8307368341971326  Accuracy :  0.7206944444444444 Time  0.09 s\nEpoch :  1  Batch :  1400  Loss :   0.818441715698157  Accuracy :  0.72375 Time  0.09 s\nEpoch :  1  Batch :  1450  Loss :   0.8076801362325405  Accuracy :  0.7268534482758621 Time  0.12 s\nEpoch :  1  Batch :  1500  Loss :   0.7971743767261505  Accuracy :  0.729375 Time  0.09 s\nEpoch :  1  Batch :  1550  Loss :   0.7896476461618177  Accuracy :  0.7314112903225807 Time  0.09 s\nEpoch :  1  Batch :  1600  Loss :   0.7794786676764488  Accuracy :  0.7340625 Time  0.09 s\nEpoch :  1  Batch :  1650  Loss :   0.7700706829627355  Accuracy :  0.7364015151515152 Time  0.09 s\nEpoch :  1  Batch :  1700  Loss :   0.7592696489919635  Accuracy :  0.7390808823529412 Time  0.09 s\nEpoch :  1  Batch :  1750  Loss :   0.7480447346993855  Accuracy :  0.7423571428571428 Time  0.09 s\nEpoch :  1  Batch :  1800  Loss :   0.7390255578441752  Accuracy :  0.7452430555555556 Time  0.09 s\nEpoch :  1  Batch :  1850  Loss :   0.7312408731918077  Accuracy :  0.7472635135135135 Time  0.09 s\nAccuracy of     0 : 41 %\nAccuracy of     1 : 54 %\nAccuracy of     2 : 75 %\nAccuracy of     3 : 81 %\nAccuracy of     4 : 85 %\nAccuracy of     5 : 77 %\nAccuracy of     6 : 81 %\nAccuracy of     7 : 86 %\nAccuracy of     8 : 85 %\nAccuracy of     9 : 77 %\nAccuracy of    10 : 84 %\nAccuracy of    11 : 81 %\nAccuracy of    12 : 55 %\nAccuracy of    13 : 78 %\nAccuracy of    14 : 79 %\nAccuracy of    15 : 88 %\nAccuracy of    16 : 84 %\nAccuracy of    17 : 83 %\nAccuracy of    18 : 92 %\nAccuracy of    19 : 91 %\nAccuracy of    20 : 55 %\nAccuracy of    21 : 94 %\nAccuracy of    22 : 82 %\nAccuracy of    23 : 85 %\nAccuracy of    24 : 34 %\nAccuracy of    25 : 69 %\nAccuracy of    26 : 92 %\nAccuracy of    27 : 90 %\nAccuracy of    28 : 48 %\nAccuracy of    29 : 93 %\nAccuracy of    30 : 89 %\nAccuracy of    31 : 45 %\nAccuracy of    32 : 86 %\nAccuracy of    33 : 65 %\nAccuracy of    34 : 88 %\nAccuracy of    35 : 68 %\nAccuracy of    36 : 78 %\nAccuracy of    37 : 67 %\nAccuracy of    38 : 60 %\nAccuracy of    39 : 70 %\nAccuracy of    40 : 87 %\nAccuracy of    41 : 82 %\nAccuracy of    42 : 74 %\nAccuracy of    43 : 88 %\nAccuracy of    44 : 89 %\nAccuracy of    45 : 90 %\nAccuracy of    46 : 56 %\nAccuracy of    47 : 55 %\nAccuracy of    48 : 86 %\nAccuracy of    49 : 83 %\nAccuracy of    50 : 39 %\nAccuracy of    51 : 57 %\nAccuracy of    52 : 60 %\nAccuracy of    53 : 87 %\nAccuracy of    54 : 52 %\nAccuracy of    55 : 89 %\nAccuracy of    56 : 87 %\nAccuracy of    57 : 52 %\nAccuracy of    58 : 91 %\nAccuracy of    59 : 65 %\nAccuracy of    60 : 64 %\nAccuracy of    61 : 56 %\n[1 epoch] Accuracy of the network on the Training images: 74 %\nEpoch :  2  Batch :  50  Loss :   0.3704436060786247  Accuracy :  0.84625 Time  0.09 s\nEpoch :  2  Batch :  100  Loss :   0.39481177791953087  Accuracy :  0.83625 Time  0.09 s\nEpoch :  2  Batch :  150  Loss :   0.39587411483128865  Accuracy :  0.83875 Time  0.09 s\nEpoch :  2  Batch :  200  Loss :   0.38477505654096605  Accuracy :  0.8434375 Time  0.09 s\nEpoch :  2  Batch :  250  Loss :   0.3896435204744339  Accuracy :  0.844 Time  0.09 s\nEpoch :  2  Batch :  300  Loss :   0.3903119175632795  Accuracy :  0.8447916666666667 Time  0.09 s\nEpoch :  2  Batch :  350  Loss :   0.3888295469113759  Accuracy :  0.8446428571428571 Time  0.1 s\nEpoch :  2  Batch :  400  Loss :   0.3953679733723402  Accuracy :  0.841875 Time  0.09 s\nEpoch :  2  Batch :  450  Loss :   0.3979228068391482  Accuracy :  0.8413888888888889 Time  0.09 s\nEpoch :  2  Batch :  500  Loss :   0.4051758206784725  Accuracy :  0.838 Time  0.09 s\nEpoch :  2  Batch :  550  Loss :   0.4071164305643602  Accuracy :  0.8360227272727273 Time  0.09 s\nEpoch :  2  Batch :  600  Loss :   0.4043762700756391  Accuracy :  0.8369791666666667 Time  0.09 s\nEpoch :  2  Batch :  650  Loss :   0.40299171679295026  Accuracy :  0.8383653846153846 Time  0.1 s\nEpoch :  2  Batch :  700  Loss :   0.4016280330717564  Accuracy :  0.83875 Time  0.09 s\nEpoch :  2  Batch :  750  Loss :   0.40308644658327103  Accuracy :  0.8380833333333333 Time  0.09 s\nEpoch :  2  Batch :  800  Loss :   0.4014271333999932  Accuracy :  0.838046875 Time  0.09 s\nEpoch :  2  Batch :  850  Loss :   0.4031360873229363  Accuracy :  0.8380147058823529 Time  0.09 s\nEpoch :  2  Batch :  900  Loss :   0.4068345899383227  Accuracy :  0.8360416666666667 Time  0.09 s\nEpoch :  2  Batch :  950  Loss :   0.40778668467935764  Accuracy :  0.8355921052631579 Time  0.09 s\nEpoch :  2  Batch :  1000  Loss :   0.40700393180549144  Accuracy :  0.836625 Time  0.09 s\nEpoch :  2  Batch :  1050  Loss :   0.405286344786485  Accuracy :  0.8376190476190476 Time  0.09 s\nEpoch :  2  Batch :  1100  Loss :   0.40473162378777156  Accuracy :  0.8385795454545455 Time  0.09 s\nEpoch :  2  Batch :  1150  Loss :   0.4027421682813893  Accuracy :  0.8398369565217392 Time  0.09 s\nEpoch :  2  Batch :  1200  Loss :   0.39970344947030145  Accuracy :  0.84125 Time  0.09 s\nEpoch :  2  Batch :  1250  Loss :   0.39848754826784133  Accuracy :  0.8416 Time  0.09 s\nEpoch :  2  Batch :  1300  Loss :   0.3971730850178462  Accuracy :  0.8419230769230769 Time  0.09 s\nEpoch :  2  Batch :  1350  Loss :   0.39921282763834354  Accuracy :  0.8411574074074074 Time  0.09 s\nEpoch :  2  Batch :  1400  Loss :   0.39811818047293596  Accuracy :  0.8413392857142857 Time  0.09 s\nEpoch :  2  Batch :  1450  Loss :   0.39707541787418826  Accuracy :  0.8416810344827587 Time  0.09 s\nEpoch :  2  Batch :  1500  Loss :   0.39577378923694295  Accuracy :  0.8419166666666666 Time  0.09 s\nEpoch :  2  Batch :  1550  Loss :   0.39708374874245733  Accuracy :  0.8421774193548387 Time  0.09 s\nEpoch :  2  Batch :  1600  Loss :   0.39532556138932706  Accuracy :  0.84328125 Time  0.09 s\nEpoch :  2  Batch :  1650  Loss :   0.3951910444461938  Accuracy :  0.8434469696969698 Time  0.1 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  2  Batch :  1700  Loss :   0.39407436811748675  Accuracy :  0.8440441176470588 Time  0.09 s\nEpoch :  2  Batch :  1750  Loss :   0.3942440291047096  Accuracy :  0.8438214285714286 Time  0.09 s\nEpoch :  2  Batch :  1800  Loss :   0.39511084386044076  Accuracy :  0.8439930555555556 Time  0.09 s\nEpoch :  2  Batch :  1850  Loss :   0.3950813146861824  Accuracy :  0.8439527027027027 Time  0.09 s\nAccuracy of     0 : 54 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 84 %\nAccuracy of     3 : 94 %\nAccuracy of     4 : 93 %\nAccuracy of     5 : 93 %\nAccuracy of     6 : 91 %\nAccuracy of     7 : 96 %\nAccuracy of     8 : 94 %\nAccuracy of     9 : 90 %\nAccuracy of    10 : 94 %\nAccuracy of    11 : 91 %\nAccuracy of    12 : 75 %\nAccuracy of    13 : 87 %\nAccuracy of    14 : 89 %\nAccuracy of    15 : 94 %\nAccuracy of    16 : 95 %\nAccuracy of    17 : 94 %\nAccuracy of    18 : 97 %\nAccuracy of    19 : 96 %\nAccuracy of    20 : 73 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 95 %\nAccuracy of    23 : 96 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 79 %\nAccuracy of    26 : 96 %\nAccuracy of    27 : 97 %\nAccuracy of    28 : 59 %\nAccuracy of    29 : 98 %\nAccuracy of    30 : 94 %\nAccuracy of    31 : 53 %\nAccuracy of    32 : 95 %\nAccuracy of    33 : 71 %\nAccuracy of    34 : 95 %\nAccuracy of    35 : 73 %\nAccuracy of    36 : 91 %\nAccuracy of    37 : 76 %\nAccuracy of    38 : 77 %\nAccuracy of    39 : 79 %\nAccuracy of    40 : 97 %\nAccuracy of    41 : 94 %\nAccuracy of    42 : 90 %\nAccuracy of    43 : 96 %\nAccuracy of    44 : 94 %\nAccuracy of    45 : 95 %\nAccuracy of    46 : 71 %\nAccuracy of    47 : 60 %\nAccuracy of    48 : 94 %\nAccuracy of    49 : 94 %\nAccuracy of    50 : 42 %\nAccuracy of    51 : 73 %\nAccuracy of    52 : 85 %\nAccuracy of    53 : 95 %\nAccuracy of    54 : 61 %\nAccuracy of    55 : 96 %\nAccuracy of    56 : 95 %\nAccuracy of    57 : 58 %\nAccuracy of    58 : 96 %\nAccuracy of    59 : 75 %\nAccuracy of    60 : 89 %\nAccuracy of    61 : 66 %\n[2 epoch] Accuracy of the network on the Training images: 84 %\nEpoch :  3  Batch :  50  Loss :   0.39254663586616517  Accuracy :  0.84625 Time  0.09 s\nEpoch :  3  Batch :  100  Loss :   0.32918880954384805  Accuracy :  0.865 Time  0.09 s\nEpoch :  3  Batch :  150  Loss :   0.33411837309598924  Accuracy :  0.8633333333333333 Time  0.09 s\nEpoch :  3  Batch :  200  Loss :   0.3466814298182726  Accuracy :  0.8621875 Time  0.09 s\nEpoch :  3  Batch :  250  Loss :   0.35151721465587615  Accuracy :  0.8585 Time  0.1 s\nEpoch :  3  Batch :  300  Loss :   0.3566550903022289  Accuracy :  0.8529166666666667 Time  0.1 s\nEpoch :  3  Batch :  350  Loss :   0.3534581853236471  Accuracy :  0.8539285714285715 Time  0.09 s\nEpoch :  3  Batch :  400  Loss :   0.34979090325534345  Accuracy :  0.85625 Time  0.09 s\nEpoch :  3  Batch :  450  Loss :   0.3476525499092208  Accuracy :  0.8590277777777777 Time  0.09 s\nEpoch :  3  Batch :  500  Loss :   0.34316196748614314  Accuracy :  0.860625 Time  0.09 s\nEpoch :  3  Batch :  550  Loss :   0.3415933351083235  Accuracy :  0.8625 Time  0.09 s\nEpoch :  3  Batch :  600  Loss :   0.34257366274793943  Accuracy :  0.8619791666666666 Time  0.1 s\nEpoch :  3  Batch :  650  Loss :   0.3427562472682733  Accuracy :  0.8614423076923077 Time  0.09 s\nEpoch :  3  Batch :  700  Loss :   0.348620917818376  Accuracy :  0.8597321428571428 Time  0.09 s\nEpoch :  3  Batch :  750  Loss :   0.3473712160984675  Accuracy :  0.85975 Time  0.09 s\nEpoch :  3  Batch :  800  Loss :   0.34444925356656314  Accuracy :  0.860859375 Time  0.09 s\nEpoch :  3  Batch :  850  Loss :   0.3443981163641986  Accuracy :  0.860735294117647 Time  0.09 s\nEpoch :  3  Batch :  900  Loss :   0.344757641851902  Accuracy :  0.8595833333333334 Time  0.09 s\nEpoch :  3  Batch :  950  Loss :   0.34356843959344063  Accuracy :  0.860328947368421 Time  0.11 s\nEpoch :  3  Batch :  1000  Loss :   0.34485150265693665  Accuracy :  0.8596875 Time  0.09 s\nEpoch :  3  Batch :  1050  Loss :   0.34489449557803925  Accuracy :  0.8593452380952381 Time  0.09 s\nEpoch :  3  Batch :  1100  Loss :   0.3431691238148646  Accuracy :  0.86 Time  0.09 s\nEpoch :  3  Batch :  1150  Loss :   0.34086147837016895  Accuracy :  0.8607608695652174 Time  0.1 s\nEpoch :  3  Batch :  1200  Loss :   0.3418147588521242  Accuracy :  0.8598958333333333 Time  0.09 s\nEpoch :  3  Batch :  1250  Loss :   0.34499857156276703  Accuracy :  0.85865 Time  0.09 s\nEpoch :  3  Batch :  1300  Loss :   0.3438648760777253  Accuracy :  0.8586057692307693 Time  0.09 s\nEpoch :  3  Batch :  1350  Loss :   0.34589942585538935  Accuracy :  0.8585185185185186 Time  0.09 s\nEpoch :  3  Batch :  1400  Loss :   0.34533039592206477  Accuracy :  0.8585267857142858 Time  0.09 s\nEpoch :  3  Batch :  1450  Loss :   0.34310589054535173  Accuracy :  0.8591379310344828 Time  0.09 s\nEpoch :  3  Batch :  1500  Loss :   0.3443539042671522  Accuracy :  0.8587916666666666 Time  0.09 s\nEpoch :  3  Batch :  1550  Loss :   0.3435033314939468  Accuracy :  0.8594354838709677 Time  0.09 s\nEpoch :  3  Batch :  1600  Loss :   0.3445066696964204  Accuracy :  0.8595703125 Time  0.09 s\nEpoch :  3  Batch :  1650  Loss :   0.34477782794020395  Accuracy :  0.8597727272727272 Time  0.09 s\nEpoch :  3  Batch :  1700  Loss :   0.34486422885866724  Accuracy :  0.8594485294117648 Time  0.09 s\nEpoch :  3  Batch :  1750  Loss :   0.3458710440652711  Accuracy :  0.8585357142857143 Time  0.09 s\nEpoch :  3  Batch :  1800  Loss :   0.34488786339759825  Accuracy :  0.8591666666666666 Time  0.09 s\nEpoch :  3  Batch :  1850  Loss :   0.3442535202084361  Accuracy :  0.8599662162162162 Time  0.1 s\nAccuracy of     0 : 54 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 87 %\nAccuracy of     3 : 96 %\nAccuracy of     4 : 95 %\nAccuracy of     5 : 93 %\nAccuracy of     6 : 92 %\nAccuracy of     7 : 96 %\nAccuracy of     8 : 97 %\nAccuracy of     9 : 91 %\nAccuracy of    10 : 94 %\nAccuracy of    11 : 93 %\nAccuracy of    12 : 78 %\nAccuracy of    13 : 89 %\nAccuracy of    14 : 93 %\nAccuracy of    15 : 95 %\nAccuracy of    16 : 95 %\nAccuracy of    17 : 95 %\nAccuracy of    18 : 98 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 79 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 95 %\nAccuracy of    23 : 96 %\nAccuracy of    24 : 34 %\nAccuracy of    25 : 79 %\nAccuracy of    26 : 98 %\nAccuracy of    27 : 97 %\nAccuracy of    28 : 64 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 94 %\nAccuracy of    31 : 52 %\nAccuracy of    32 : 96 %\nAccuracy of    33 : 77 %\nAccuracy of    34 : 96 %\nAccuracy of    35 : 78 %\nAccuracy of    36 : 93 %\nAccuracy of    37 : 79 %\nAccuracy of    38 : 80 %\nAccuracy of    39 : 82 %\nAccuracy of    40 : 97 %\nAccuracy of    41 : 93 %\nAccuracy of    42 : 94 %\nAccuracy of    43 : 97 %\nAccuracy of    44 : 95 %\nAccuracy of    45 : 96 %\nAccuracy of    46 : 79 %\nAccuracy of    47 : 60 %\nAccuracy of    48 : 95 %\nAccuracy of    49 : 95 %\nAccuracy of    50 : 41 %\nAccuracy of    51 : 75 %\nAccuracy of    52 : 89 %\nAccuracy of    53 : 96 %\nAccuracy of    54 : 62 %\nAccuracy of    55 : 97 %\nAccuracy of    56 : 96 %\nAccuracy of    57 : 62 %\nAccuracy of    58 : 96 %\nAccuracy of    59 : 79 %\nAccuracy of    60 : 90 %\nAccuracy of    61 : 68 %\n[3 epoch] Accuracy of the network on the Training images: 86 %\nEpoch :  4  Batch :  50  Loss :   0.3310039609670639  Accuracy :  0.86125 Time  0.09 s\nEpoch :  4  Batch :  100  Loss :   0.31017570734024047  Accuracy :  0.878125 Time  0.09 s\nEpoch :  4  Batch :  150  Loss :   0.3099068786700567  Accuracy :  0.8808333333333334 Time  0.09 s\nEpoch :  4  Batch :  200  Loss :   0.3045225340873003  Accuracy :  0.88 Time  0.09 s\nEpoch :  4  Batch :  250  Loss :   0.3010367733836174  Accuracy :  0.88075 Time  0.1 s\nEpoch :  4  Batch :  300  Loss :   0.3004964459439119  Accuracy :  0.8808333333333334 Time  0.09 s\nEpoch :  4  Batch :  350  Loss :   0.30696533863033565  Accuracy :  0.8767857142857143 Time  0.12 s\nEpoch :  4  Batch :  400  Loss :   0.31857688691467045  Accuracy :  0.87203125 Time  0.09 s\nEpoch :  4  Batch :  450  Loss :   0.3177038149370088  Accuracy :  0.8718055555555555 Time  0.09 s\nEpoch :  4  Batch :  500  Loss :   0.3149489405453205  Accuracy :  0.873375 Time  0.1 s\nEpoch :  4  Batch :  550  Loss :   0.31367439080368387  Accuracy :  0.8746590909090909 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  4  Batch :  600  Loss :   0.31234617307782175  Accuracy :  0.8748958333333333 Time  0.09 s\nEpoch :  4  Batch :  650  Loss :   0.3143459295997253  Accuracy :  0.874423076923077 Time  0.09 s\nEpoch :  4  Batch :  700  Loss :   0.31255802407860755  Accuracy :  0.8754464285714286 Time  0.09 s\nEpoch :  4  Batch :  750  Loss :   0.3116432680288951  Accuracy :  0.8755 Time  0.09 s\nEpoch :  4  Batch :  800  Loss :   0.3084970297850668  Accuracy :  0.875390625 Time  0.09 s\nEpoch :  4  Batch :  850  Loss :   0.30607299126246396  Accuracy :  0.8766911764705883 Time  0.09 s\nEpoch :  4  Batch :  900  Loss :   0.30546734487016997  Accuracy :  0.8765277777777778 Time  0.09 s\nEpoch :  4  Batch :  950  Loss :   0.3061465007223581  Accuracy :  0.8761842105263158 Time  0.09 s\nEpoch :  4  Batch :  1000  Loss :   0.30790717448294164  Accuracy :  0.875375 Time  0.09 s\nEpoch :  4  Batch :  1050  Loss :   0.307978118956089  Accuracy :  0.8748809523809524 Time  0.11 s\nEpoch :  4  Batch :  1100  Loss :   0.30936897326599466  Accuracy :  0.8747159090909091 Time  0.09 s\nEpoch :  4  Batch :  1150  Loss :   0.308507581534593  Accuracy :  0.8752173913043478 Time  0.09 s\nEpoch :  4  Batch :  1200  Loss :   0.3094641287624836  Accuracy :  0.8744791666666667 Time  0.09 s\nEpoch :  4  Batch :  1250  Loss :   0.3100230659723282  Accuracy :  0.8742 Time  0.09 s\nEpoch :  4  Batch :  1300  Loss :   0.31207597915942853  Accuracy :  0.8728846153846154 Time  0.09 s\nEpoch :  4  Batch :  1350  Loss :   0.3128659610615836  Accuracy :  0.8724537037037037 Time  0.09 s\nEpoch :  4  Batch :  1400  Loss :   0.313777561123882  Accuracy :  0.8715625 Time  0.09 s\nEpoch :  4  Batch :  1450  Loss :   0.31438443640182756  Accuracy :  0.8707758620689655 Time  0.09 s\nEpoch :  4  Batch :  1500  Loss :   0.31484799581766126  Accuracy :  0.871 Time  0.09 s\nEpoch :  4  Batch :  1550  Loss :   0.3147935833469514  Accuracy :  0.8708870967741935 Time  0.09 s\nEpoch :  4  Batch :  1600  Loss :   0.3139931624382734  Accuracy :  0.8708984375 Time  0.09 s\nEpoch :  4  Batch :  1650  Loss :   0.3126567723714944  Accuracy :  0.8713636363636363 Time  0.09 s\nEpoch :  4  Batch :  1700  Loss :   0.31314150717328576  Accuracy :  0.8710661764705883 Time  0.09 s\nEpoch :  4  Batch :  1750  Loss :   0.3144010182959693  Accuracy :  0.8705 Time  0.09 s\nEpoch :  4  Batch :  1800  Loss :   0.31513029461933506  Accuracy :  0.8704513888888888 Time  0.09 s\nEpoch :  4  Batch :  1850  Loss :   0.31514547772504187  Accuracy :  0.870472972972973 Time  0.09 s\nAccuracy of     0 : 54 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 88 %\nAccuracy of     3 : 97 %\nAccuracy of     4 : 96 %\nAccuracy of     5 : 95 %\nAccuracy of     6 : 94 %\nAccuracy of     7 : 96 %\nAccuracy of     8 : 95 %\nAccuracy of     9 : 91 %\nAccuracy of    10 : 95 %\nAccuracy of    11 : 94 %\nAccuracy of    12 : 81 %\nAccuracy of    13 : 92 %\nAccuracy of    14 : 94 %\nAccuracy of    15 : 95 %\nAccuracy of    16 : 96 %\nAccuracy of    17 : 97 %\nAccuracy of    18 : 98 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 85 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 96 %\nAccuracy of    23 : 97 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 80 %\nAccuracy of    26 : 98 %\nAccuracy of    27 : 98 %\nAccuracy of    28 : 68 %\nAccuracy of    29 : 98 %\nAccuracy of    30 : 96 %\nAccuracy of    31 : 59 %\nAccuracy of    32 : 96 %\nAccuracy of    33 : 79 %\nAccuracy of    34 : 96 %\nAccuracy of    35 : 80 %\nAccuracy of    36 : 93 %\nAccuracy of    37 : 82 %\nAccuracy of    38 : 86 %\nAccuracy of    39 : 84 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 94 %\nAccuracy of    42 : 94 %\nAccuracy of    43 : 97 %\nAccuracy of    44 : 95 %\nAccuracy of    45 : 96 %\nAccuracy of    46 : 86 %\nAccuracy of    47 : 59 %\nAccuracy of    48 : 95 %\nAccuracy of    49 : 95 %\nAccuracy of    50 : 38 %\nAccuracy of    51 : 77 %\nAccuracy of    52 : 90 %\nAccuracy of    53 : 96 %\nAccuracy of    54 : 64 %\nAccuracy of    55 : 98 %\nAccuracy of    56 : 96 %\nAccuracy of    57 : 61 %\nAccuracy of    58 : 96 %\nAccuracy of    59 : 78 %\nAccuracy of    60 : 94 %\nAccuracy of    61 : 68 %\n[4 epoch] Accuracy of the network on the Training images: 87 %\nEpoch :  5  Batch :  50  Loss :   0.21449462234973907  Accuracy :  0.90125 Time  0.09 s\nEpoch :  5  Batch :  100  Loss :   0.22265625685453416  Accuracy :  0.9025 Time  0.09 s\nEpoch :  5  Batch :  150  Loss :   0.24278779923915864  Accuracy :  0.9020833333333333 Time  0.09 s\nEpoch :  5  Batch :  200  Loss :   0.26660252206027507  Accuracy :  0.893125 Time  0.09 s\nEpoch :  5  Batch :  250  Loss :   0.2721535145640373  Accuracy :  0.8935 Time  0.09 s\nEpoch :  5  Batch :  300  Loss :   0.2675518061220646  Accuracy :  0.8933333333333333 Time  0.09 s\nEpoch :  5  Batch :  350  Loss :   0.26772820983614243  Accuracy :  0.8933928571428571 Time  0.09 s\nEpoch :  5  Batch :  400  Loss :   0.268227012231946  Accuracy :  0.8928125 Time  0.09 s\nEpoch :  5  Batch :  450  Loss :   0.27140980064868925  Accuracy :  0.8905555555555555 Time  0.09 s\nEpoch :  5  Batch :  500  Loss :   0.2744164899587631  Accuracy :  0.888625 Time  0.09 s\nEpoch :  5  Batch :  550  Loss :   0.2794709615544839  Accuracy :  0.8855681818181819 Time  0.09 s\nEpoch :  5  Batch :  600  Loss :   0.27855087953309215  Accuracy :  0.8847916666666666 Time  0.09 s\nEpoch :  5  Batch :  650  Loss :   0.2816492003431687  Accuracy :  0.8832692307692308 Time  0.09 s\nEpoch :  5  Batch :  700  Loss :   0.281049877937351  Accuracy :  0.88375 Time  0.09 s\nEpoch :  5  Batch :  750  Loss :   0.28232053196430207  Accuracy :  0.8836666666666667 Time  0.09 s\nEpoch :  5  Batch :  800  Loss :   0.2830702628195286  Accuracy :  0.883125 Time  0.09 s\nEpoch :  5  Batch :  850  Loss :   0.282189681039137  Accuracy :  0.8831617647058824 Time  0.09 s\nEpoch :  5  Batch :  900  Loss :   0.28484026590983075  Accuracy :  0.8822916666666667 Time  0.09 s\nEpoch :  5  Batch :  950  Loss :   0.28433263290869565  Accuracy :  0.8824342105263158 Time  0.09 s\nEpoch :  5  Batch :  1000  Loss :   0.2844602024704218  Accuracy :  0.8821875 Time  0.09 s\nEpoch :  5  Batch :  1050  Loss :   0.285357161292008  Accuracy :  0.8822619047619048 Time  0.09 s\nEpoch :  5  Batch :  1100  Loss :   0.284485625367273  Accuracy :  0.8825568181818182 Time  0.09 s\nEpoch :  5  Batch :  1150  Loss :   0.28640564782463984  Accuracy :  0.8818478260869566 Time  0.09 s\nEpoch :  5  Batch :  1200  Loss :   0.2861184663698077  Accuracy :  0.8818229166666667 Time  0.1 s\nEpoch :  5  Batch :  1250  Loss :   0.2835242716550827  Accuracy :  0.88335 Time  0.09 s\nEpoch :  5  Batch :  1300  Loss :   0.2836612458756337  Accuracy :  0.8829326923076923 Time  0.09 s\nEpoch :  5  Batch :  1350  Loss :   0.2842015169836857  Accuracy :  0.882962962962963 Time  0.09 s\nEpoch :  5  Batch :  1400  Loss :   0.28383559497339383  Accuracy :  0.883125 Time  0.09 s\nEpoch :  5  Batch :  1450  Loss :   0.2844464517872909  Accuracy :  0.8831896551724138 Time  0.09 s\nEpoch :  5  Batch :  1500  Loss :   0.28479193047682444  Accuracy :  0.882375 Time  0.09 s\nEpoch :  5  Batch :  1550  Loss :   0.28402129541481697  Accuracy :  0.8824193548387097 Time  0.09 s\nEpoch :  5  Batch :  1600  Loss :   0.2839480330888182  Accuracy :  0.8825 Time  0.09 s\nEpoch :  5  Batch :  1650  Loss :   0.28530709565588924  Accuracy :  0.8820833333333333 Time  0.09 s\nEpoch :  5  Batch :  1700  Loss :   0.2853569787916015  Accuracy :  0.8821691176470589 Time  0.1 s\nEpoch :  5  Batch :  1750  Loss :   0.28625254654884336  Accuracy :  0.8821428571428571 Time  0.09 s\nEpoch :  5  Batch :  1800  Loss :   0.28506820546256173  Accuracy :  0.8827083333333333 Time  0.09 s\nEpoch :  5  Batch :  1850  Loss :   0.28513996630907057  Accuracy :  0.882804054054054 Time  0.09 s\nAccuracy of     0 : 53 %\nAccuracy of     1 : 60 %\nAccuracy of     2 : 90 %\nAccuracy of     3 : 97 %\nAccuracy of     4 : 96 %\nAccuracy of     5 : 97 %\nAccuracy of     6 : 95 %\nAccuracy of     7 : 97 %\nAccuracy of     8 : 97 %\nAccuracy of     9 : 93 %\nAccuracy of    10 : 97 %\nAccuracy of    11 : 97 %\nAccuracy of    12 : 87 %\nAccuracy of    13 : 92 %\nAccuracy of    14 : 93 %\nAccuracy of    15 : 96 %\nAccuracy of    16 : 97 %\nAccuracy of    17 : 97 %\nAccuracy of    18 : 98 %\nAccuracy of    19 : 98 %\nAccuracy of    20 : 88 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 95 %\nAccuracy of    23 : 97 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 85 %\nAccuracy of    26 : 98 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 66 %\nAccuracy of    29 : 98 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 61 %\nAccuracy of    32 : 96 %\nAccuracy of    33 : 81 %\nAccuracy of    34 : 97 %\nAccuracy of    35 : 82 %\nAccuracy of    36 : 94 %\nAccuracy of    37 : 82 %\nAccuracy of    38 : 88 %\nAccuracy of    39 : 85 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 95 %\nAccuracy of    42 : 95 %\nAccuracy of    43 : 98 %\nAccuracy of    44 : 97 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 89 %\nAccuracy of    47 : 61 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 97 %\nAccuracy of    50 : 43 %\nAccuracy of    51 : 79 %\nAccuracy of    52 : 94 %\nAccuracy of    53 : 97 %\nAccuracy of    54 : 64 %\nAccuracy of    55 : 98 %\nAccuracy of    56 : 97 %\nAccuracy of    57 : 64 %\nAccuracy of    58 : 96 %\nAccuracy of    59 : 78 %\nAccuracy of    60 : 92 %\nAccuracy of    61 : 71 %\n[5 epoch] Accuracy of the network on the Training images: 88 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  6  Batch :  50  Loss :   0.260071736574173  Accuracy :  0.88875 Time  0.09 s\nEpoch :  6  Batch :  100  Loss :   0.26425350427627564  Accuracy :  0.8875 Time  0.09 s\nEpoch :  6  Batch :  150  Loss :   0.26675413171450296  Accuracy :  0.89125 Time  0.09 s\nEpoch :  6  Batch :  200  Loss :   0.26599874258041384  Accuracy :  0.889375 Time  0.09 s\nEpoch :  6  Batch :  250  Loss :   0.28070183312892916  Accuracy :  0.8855 Time  0.09 s\nEpoch :  6  Batch :  300  Loss :   0.27785718520482383  Accuracy :  0.886875 Time  0.09 s\nEpoch :  6  Batch :  350  Loss :   0.26946379218782696  Accuracy :  0.8896428571428572 Time  0.09 s\nEpoch :  6  Batch :  400  Loss :   0.267003882676363  Accuracy :  0.88875 Time  0.09 s\nEpoch :  6  Batch :  450  Loss :   0.26316511730353037  Accuracy :  0.8906944444444445 Time  0.09 s\nEpoch :  6  Batch :  500  Loss :   0.2637500759959221  Accuracy :  0.890375 Time  0.09 s\nEpoch :  6  Batch :  550  Loss :   0.2650604022632946  Accuracy :  0.8889772727272728 Time  0.09 s\nEpoch :  6  Batch :  600  Loss :   0.2682943488409122  Accuracy :  0.8880208333333334 Time  0.09 s\nEpoch :  6  Batch :  650  Loss :   0.2673289784789085  Accuracy :  0.8883653846153846 Time  0.09 s\nEpoch :  6  Batch :  700  Loss :   0.2713354497935091  Accuracy :  0.8872321428571428 Time  0.09 s\nEpoch :  6  Batch :  750  Loss :   0.2704697236617406  Accuracy :  0.8879166666666667 Time  0.09 s\nEpoch :  6  Batch :  800  Loss :   0.2716989754885435  Accuracy :  0.887734375 Time  0.09 s\nEpoch :  6  Batch :  850  Loss :   0.27030232871279997  Accuracy :  0.8880882352941176 Time  0.09 s\nEpoch :  6  Batch :  900  Loss :   0.27046938757101696  Accuracy :  0.8870138888888889 Time  0.09 s\nEpoch :  6  Batch :  950  Loss :   0.27030071528334365  Accuracy :  0.8867763157894737 Time  0.09 s\nEpoch :  6  Batch :  1000  Loss :   0.26980604270100594  Accuracy :  0.8875 Time  0.09 s\nEpoch :  6  Batch :  1050  Loss :   0.26976377092656634  Accuracy :  0.8873809523809524 Time  0.09 s\nEpoch :  6  Batch :  1100  Loss :   0.2701722248711369  Accuracy :  0.8869318181818182 Time  0.09 s\nEpoch :  6  Batch :  1150  Loss :   0.26968338006216547  Accuracy :  0.8870108695652174 Time  0.09 s\nEpoch :  6  Batch :  1200  Loss :   0.2708650671814879  Accuracy :  0.88703125 Time  0.09 s\nEpoch :  6  Batch :  1250  Loss :   0.2721594124913216  Accuracy :  0.8864 Time  0.09 s\nEpoch :  6  Batch :  1300  Loss :   0.2729559069298781  Accuracy :  0.8857211538461538 Time  0.09 s\nEpoch :  6  Batch :  1350  Loss :   0.2717398088618561  Accuracy :  0.8860185185185185 Time  0.09 s\nEpoch :  6  Batch :  1400  Loss :   0.27170572013727257  Accuracy :  0.8861160714285714 Time  0.09 s\nEpoch :  6  Batch :  1450  Loss :   0.2716800584978071  Accuracy :  0.8858620689655172 Time  0.09 s\nEpoch :  6  Batch :  1500  Loss :   0.27165583163499835  Accuracy :  0.8855 Time  0.09 s\nEpoch :  6  Batch :  1550  Loss :   0.2710178529831671  Accuracy :  0.8864112903225806 Time  0.09 s\nEpoch :  6  Batch :  1600  Loss :   0.2708253190666437  Accuracy :  0.886484375 Time  0.09 s\nEpoch :  6  Batch :  1650  Loss :   0.2704164031238267  Accuracy :  0.886439393939394 Time  0.09 s\nEpoch :  6  Batch :  1700  Loss :   0.2698693169215146  Accuracy :  0.8861764705882353 Time  0.1 s\nEpoch :  6  Batch :  1750  Loss :   0.2698681744847979  Accuracy :  0.8859642857142858 Time  0.09 s\nEpoch :  6  Batch :  1800  Loss :   0.26992484194537003  Accuracy :  0.8859722222222223 Time  0.09 s\nEpoch :  6  Batch :  1850  Loss :   0.2705682917946094  Accuracy :  0.8859797297297297 Time  0.09 s\nAccuracy of     0 : 54 %\nAccuracy of     1 : 63 %\nAccuracy of     2 : 90 %\nAccuracy of     3 : 98 %\nAccuracy of     4 : 97 %\nAccuracy of     5 : 97 %\nAccuracy of     6 : 95 %\nAccuracy of     7 : 96 %\nAccuracy of     8 : 96 %\nAccuracy of     9 : 93 %\nAccuracy of    10 : 97 %\nAccuracy of    11 : 96 %\nAccuracy of    12 : 87 %\nAccuracy of    13 : 92 %\nAccuracy of    14 : 95 %\nAccuracy of    15 : 97 %\nAccuracy of    16 : 97 %\nAccuracy of    17 : 96 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 90 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 96 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 83 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 97 %\nAccuracy of    28 : 68 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 58 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 82 %\nAccuracy of    34 : 97 %\nAccuracy of    35 : 82 %\nAccuracy of    36 : 95 %\nAccuracy of    37 : 83 %\nAccuracy of    38 : 90 %\nAccuracy of    39 : 88 %\nAccuracy of    40 : 97 %\nAccuracy of    41 : 95 %\nAccuracy of    42 : 95 %\nAccuracy of    43 : 98 %\nAccuracy of    44 : 97 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 92 %\nAccuracy of    47 : 62 %\nAccuracy of    48 : 96 %\nAccuracy of    49 : 96 %\nAccuracy of    50 : 40 %\nAccuracy of    51 : 79 %\nAccuracy of    52 : 93 %\nAccuracy of    53 : 97 %\nAccuracy of    54 : 66 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 97 %\nAccuracy of    57 : 64 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 79 %\nAccuracy of    60 : 90 %\nAccuracy of    61 : 72 %\n[6 epoch] Accuracy of the network on the Training images: 88 %\nEpoch :  7  Batch :  50  Loss :   0.21392902135848998  Accuracy :  0.90125 Time  0.09 s\nEpoch :  7  Batch :  100  Loss :   0.2394042432308197  Accuracy :  0.898125 Time  0.09 s\nEpoch :  7  Batch :  150  Loss :   0.23952900091807047  Accuracy :  0.9 Time  0.09 s\nEpoch :  7  Batch :  200  Loss :   0.24422158777713776  Accuracy :  0.896875 Time  0.09 s\nEpoch :  7  Batch :  250  Loss :   0.24328875017166138  Accuracy :  0.89875 Time  0.09 s\nEpoch :  7  Batch :  300  Loss :   0.24465859284003574  Accuracy :  0.8972916666666667 Time  0.09 s\nEpoch :  7  Batch :  350  Loss :   0.24681123627083643  Accuracy :  0.8951785714285714 Time  0.09 s\nEpoch :  7  Batch :  400  Loss :   0.24682468697428703  Accuracy :  0.89578125 Time  0.09 s\nEpoch :  7  Batch :  450  Loss :   0.24553234120210013  Accuracy :  0.8956944444444445 Time  0.09 s\nEpoch :  7  Batch :  500  Loss :   0.2484784571826458  Accuracy :  0.8935 Time  0.09 s\nEpoch :  7  Batch :  550  Loss :   0.250927742421627  Accuracy :  0.8922727272727272 Time  0.09 s\nEpoch :  7  Batch :  600  Loss :   0.2501610330492258  Accuracy :  0.8932291666666666 Time  0.09 s\nEpoch :  7  Batch :  650  Loss :   0.2510253576819713  Accuracy :  0.8925961538461539 Time  0.09 s\nEpoch :  7  Batch :  700  Loss :   0.25560971389923776  Accuracy :  0.8902678571428572 Time  0.09 s\nEpoch :  7  Batch :  750  Loss :   0.2609350095788638  Accuracy :  0.8879166666666667 Time  0.09 s\nEpoch :  7  Batch :  800  Loss :   0.26161036746576427  Accuracy :  0.8878125 Time  0.09 s\nEpoch :  7  Batch :  850  Loss :   0.26011970975819754  Accuracy :  0.8888235294117647 Time  0.09 s\nEpoch :  7  Batch :  900  Loss :   0.25935628569788405  Accuracy :  0.8890277777777778 Time  0.09 s\nEpoch :  7  Batch :  950  Loss :   0.2570190942130591  Accuracy :  0.8898684210526315 Time  0.09 s\nEpoch :  7  Batch :  1000  Loss :   0.2566746128052473  Accuracy :  0.89 Time  0.09 s\nEpoch :  7  Batch :  1050  Loss :   0.25526283830404284  Accuracy :  0.8904166666666666 Time  0.09 s\nEpoch :  7  Batch :  1100  Loss :   0.2551701472970572  Accuracy :  0.8902272727272728 Time  0.09 s\nEpoch :  7  Batch :  1150  Loss :   0.2562232033714004  Accuracy :  0.8898369565217391 Time  0.09 s\nEpoch :  7  Batch :  1200  Loss :   0.2555664155508081  Accuracy :  0.8902604166666667 Time  0.09 s\nEpoch :  7  Batch :  1250  Loss :   0.2560134635806084  Accuracy :  0.88975 Time  0.09 s\nEpoch :  7  Batch :  1300  Loss :   0.255639944340174  Accuracy :  0.8897115384615385 Time  0.09 s\nEpoch :  7  Batch :  1350  Loss :   0.2572148001966653  Accuracy :  0.8892129629629629 Time  0.11 s\nEpoch :  7  Batch :  1400  Loss :   0.257284281583769  Accuracy :  0.8896428571428572 Time  0.09 s\nEpoch :  7  Batch :  1450  Loss :   0.2588075923405845  Accuracy :  0.8890086206896551 Time  0.09 s\nEpoch :  7  Batch :  1500  Loss :   0.2591004199484984  Accuracy :  0.8887916666666666 Time  0.09 s\nEpoch :  7  Batch :  1550  Loss :   0.25984156113478446  Accuracy :  0.8885887096774193 Time  0.09 s\nEpoch :  7  Batch :  1600  Loss :   0.2614121062401682  Accuracy :  0.8878515625 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  7  Batch :  1650  Loss :   0.26147774105722255  Accuracy :  0.8875757575757576 Time  0.09 s\nEpoch :  7  Batch :  1700  Loss :   0.26138488995678283  Accuracy :  0.8877941176470588 Time  0.09 s\nEpoch :  7  Batch :  1750  Loss :   0.2612600268125534  Accuracy :  0.8879285714285714 Time  0.09 s\nEpoch :  7  Batch :  1800  Loss :   0.261152284923527  Accuracy :  0.8881597222222222 Time  0.09 s\nEpoch :  7  Batch :  1850  Loss :   0.2608674893346993  Accuracy :  0.8881418918918919 Time  0.09 s\nAccuracy of     0 : 58 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 91 %\nAccuracy of     3 : 97 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 96 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 97 %\nAccuracy of     8 : 97 %\nAccuracy of     9 : 94 %\nAccuracy of    10 : 97 %\nAccuracy of    11 : 96 %\nAccuracy of    12 : 88 %\nAccuracy of    13 : 94 %\nAccuracy of    14 : 96 %\nAccuracy of    15 : 97 %\nAccuracy of    16 : 97 %\nAccuracy of    17 : 96 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 98 %\nAccuracy of    20 : 93 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 97 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 83 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 98 %\nAccuracy of    28 : 68 %\nAccuracy of    29 : 98 %\nAccuracy of    30 : 96 %\nAccuracy of    31 : 59 %\nAccuracy of    32 : 96 %\nAccuracy of    33 : 82 %\nAccuracy of    34 : 97 %\nAccuracy of    35 : 84 %\nAccuracy of    36 : 95 %\nAccuracy of    37 : 84 %\nAccuracy of    38 : 92 %\nAccuracy of    39 : 86 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 95 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 96 %\nAccuracy of    45 : 97 %\nAccuracy of    46 : 92 %\nAccuracy of    47 : 62 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 97 %\nAccuracy of    50 : 42 %\nAccuracy of    51 : 80 %\nAccuracy of    52 : 94 %\nAccuracy of    53 : 97 %\nAccuracy of    54 : 64 %\nAccuracy of    55 : 97 %\nAccuracy of    56 : 97 %\nAccuracy of    57 : 64 %\nAccuracy of    58 : 97 %\nAccuracy of    59 : 78 %\nAccuracy of    60 : 93 %\nAccuracy of    61 : 73 %\n[7 epoch] Accuracy of the network on the Training images: 88 %\nEpoch :  8  Batch :  50  Loss :   0.21706908881664277  Accuracy :  0.89875 Time  0.09 s\nEpoch :  8  Batch :  100  Loss :   0.23232975870370864  Accuracy :  0.8925 Time  0.09 s\nEpoch :  8  Batch :  150  Loss :   0.22448821981747946  Accuracy :  0.8975 Time  0.11 s\nEpoch :  8  Batch :  200  Loss :   0.22572128996253013  Accuracy :  0.8978125 Time  0.09 s\nEpoch :  8  Batch :  250  Loss :   0.22792929273843765  Accuracy :  0.89575 Time  0.09 s\nEpoch :  8  Batch :  300  Loss :   0.23556551044185955  Accuracy :  0.8958333333333334 Time  0.09 s\nEpoch :  8  Batch :  350  Loss :   0.23708702628101622  Accuracy :  0.8942857142857142 Time  0.09 s\nEpoch :  8  Batch :  400  Loss :   0.235556810349226  Accuracy :  0.89546875 Time  0.09 s\nEpoch :  8  Batch :  450  Loss :   0.2300566295782725  Accuracy :  0.8966666666666666 Time  0.09 s\nEpoch :  8  Batch :  500  Loss :   0.23318825072050095  Accuracy :  0.895625 Time  0.09 s\nEpoch :  8  Batch :  550  Loss :   0.2335675981911746  Accuracy :  0.8945454545454545 Time  0.09 s\nEpoch :  8  Batch :  600  Loss :   0.23381797338525454  Accuracy :  0.894375 Time  0.09 s\nEpoch :  8  Batch :  650  Loss :   0.23671954714334928  Accuracy :  0.8933653846153846 Time  0.09 s\nEpoch :  8  Batch :  700  Loss :   0.23318771404879435  Accuracy :  0.8952678571428572 Time  0.09 s\nEpoch :  8  Batch :  750  Loss :   0.23221230469147364  Accuracy :  0.8963333333333333 Time  0.09 s\nEpoch :  8  Batch :  800  Loss :   0.2353200683183968  Accuracy :  0.89421875 Time  0.09 s\nEpoch :  8  Batch :  850  Loss :   0.23988079335759668  Accuracy :  0.8925735294117647 Time  0.09 s\nEpoch :  8  Batch :  900  Loss :   0.2394778924021456  Accuracy :  0.8920833333333333 Time  0.09 s\nEpoch :  8  Batch :  950  Loss :   0.23924152491908324  Accuracy :  0.8920394736842105 Time  0.09 s\nEpoch :  8  Batch :  1000  Loss :   0.24220537303388118  Accuracy :  0.8915625 Time  0.09 s\nEpoch :  8  Batch :  1050  Loss :   0.24233921160300573  Accuracy :  0.8922023809523809 Time  0.09 s\nEpoch :  8  Batch :  1100  Loss :   0.24168992340564727  Accuracy :  0.8927272727272727 Time  0.09 s\nEpoch :  8  Batch :  1150  Loss :   0.2409281902987024  Accuracy :  0.8932608695652174 Time  0.09 s\nEpoch :  8  Batch :  1200  Loss :   0.23998323522508144  Accuracy :  0.8938020833333333 Time  0.09 s\nEpoch :  8  Batch :  1250  Loss :   0.2417854320526123  Accuracy :  0.89305 Time  0.1 s\nEpoch :  8  Batch :  1300  Loss :   0.24191329252261382  Accuracy :  0.8935096153846154 Time  0.09 s\nEpoch :  8  Batch :  1350  Loss :   0.2424096722845678  Accuracy :  0.89375 Time  0.09 s\nEpoch :  8  Batch :  1400  Loss :   0.24278745454336917  Accuracy :  0.8934375 Time  0.09 s\nEpoch :  8  Batch :  1450  Loss :   0.24469434985826755  Accuracy :  0.8930172413793104 Time  0.09 s\nEpoch :  8  Batch :  1500  Loss :   0.24492896530032157  Accuracy :  0.8930833333333333 Time  0.09 s\nEpoch :  8  Batch :  1550  Loss :   0.24451632375678709  Accuracy :  0.8932661290322581 Time  0.11 s\nEpoch :  8  Batch :  1600  Loss :   0.24479512679390608  Accuracy :  0.8930859375 Time  0.09 s\nEpoch :  8  Batch :  1650  Loss :   0.24570968394026613  Accuracy :  0.8924621212121212 Time  0.09 s\nEpoch :  8  Batch :  1700  Loss :   0.24587924839819178  Accuracy :  0.8925 Time  0.09 s\nEpoch :  8  Batch :  1750  Loss :   0.24477731558254787  Accuracy :  0.8930714285714285 Time  0.09 s\nEpoch :  8  Batch :  1800  Loss :   0.2450673867099815  Accuracy :  0.8930208333333334 Time  0.09 s\nEpoch :  8  Batch :  1850  Loss :   0.24618001544797743  Accuracy :  0.8928716216216216 Time  0.09 s\nAccuracy of     0 : 55 %\nAccuracy of     1 : 63 %\nAccuracy of     2 : 92 %\nAccuracy of     3 : 98 %\nAccuracy of     4 : 97 %\nAccuracy of     5 : 97 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 99 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 94 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 96 %\nAccuracy of    12 : 89 %\nAccuracy of    13 : 94 %\nAccuracy of    14 : 96 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 97 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 93 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 97 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 84 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 69 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 58 %\nAccuracy of    32 : 96 %\nAccuracy of    33 : 80 %\nAccuracy of    34 : 98 %\nAccuracy of    35 : 83 %\nAccuracy of    36 : 96 %\nAccuracy of    37 : 85 %\nAccuracy of    38 : 92 %\nAccuracy of    39 : 87 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 97 %\nAccuracy of    42 : 95 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 93 %\nAccuracy of    47 : 62 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 42 %\nAccuracy of    51 : 83 %\nAccuracy of    52 : 93 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 67 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 61 %\nAccuracy of    58 : 96 %\nAccuracy of    59 : 79 %\nAccuracy of    60 : 94 %\nAccuracy of    61 : 74 %\n[8 epoch] Accuracy of the network on the Training images: 89 %\nEpoch :  9  Batch :  50  Loss :   0.1923022174835205  Accuracy :  0.9025 Time  0.09 s\nEpoch :  9  Batch :  100  Loss :   0.20929982244968415  Accuracy :  0.8925 Time  0.09 s\nEpoch :  9  Batch :  150  Loss :   0.21158321489890417  Accuracy :  0.8970833333333333 Time  0.11 s\nEpoch :  9  Batch :  200  Loss :   0.20574221931397915  Accuracy :  0.901875 Time  0.09 s\nEpoch :  9  Batch :  250  Loss :   0.2149149621129036  Accuracy :  0.90175 Time  0.09 s\nEpoch :  9  Batch :  300  Loss :   0.2207146721581618  Accuracy :  0.9008333333333334 Time  0.09 s\nEpoch :  9  Batch :  350  Loss :   0.2229713487625122  Accuracy :  0.8996428571428572 Time  0.09 s\nEpoch :  9  Batch :  400  Loss :   0.22495517306029797  Accuracy :  0.90046875 Time  0.09 s\nEpoch :  9  Batch :  450  Loss :   0.2259088134103351  Accuracy :  0.9004166666666666 Time  0.1 s\nEpoch :  9  Batch :  500  Loss :   0.22325508397817612  Accuracy :  0.901 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  9  Batch :  550  Loss :   0.2236461349508979  Accuracy :  0.9018181818181819 Time  0.09 s\nEpoch :  9  Batch :  600  Loss :   0.22201303482055665  Accuracy :  0.9025 Time  0.09 s\nEpoch :  9  Batch :  650  Loss :   0.22063247121297397  Accuracy :  0.901826923076923 Time  0.09 s\nEpoch :  9  Batch :  700  Loss :   0.22409921250173023  Accuracy :  0.9008928571428572 Time  0.09 s\nEpoch :  9  Batch :  750  Loss :   0.22612714449564617  Accuracy :  0.90025 Time  0.09 s\nEpoch :  9  Batch :  800  Loss :   0.22796004775911569  Accuracy :  0.899921875 Time  0.09 s\nEpoch :  9  Batch :  850  Loss :   0.22728108072982114  Accuracy :  0.9002941176470588 Time  0.09 s\nEpoch :  9  Batch :  900  Loss :   0.22677443659967847  Accuracy :  0.9003472222222222 Time  0.09 s\nEpoch :  9  Batch :  950  Loss :   0.22831835597753525  Accuracy :  0.9 Time  0.09 s\nEpoch :  9  Batch :  1000  Loss :   0.23172337175905705  Accuracy :  0.898 Time  0.12 s\nEpoch :  9  Batch :  1050  Loss :   0.23353397522653851  Accuracy :  0.8972619047619048 Time  0.09 s\nEpoch :  9  Batch :  1100  Loss :   0.23241542149673808  Accuracy :  0.8973863636363636 Time  0.09 s\nEpoch :  9  Batch :  1150  Loss :   0.23219475494778674  Accuracy :  0.8977717391304347 Time  0.09 s\nEpoch :  9  Batch :  1200  Loss :   0.23118341885507107  Accuracy :  0.898125 Time  0.09 s\nEpoch :  9  Batch :  1250  Loss :   0.23145337562561036  Accuracy :  0.89765 Time  0.09 s\nEpoch :  9  Batch :  1300  Loss :   0.23256825199493994  Accuracy :  0.8972115384615384 Time  0.09 s\nEpoch :  9  Batch :  1350  Loss :   0.23353548445083477  Accuracy :  0.8970833333333333 Time  0.09 s\nEpoch :  9  Batch :  1400  Loss :   0.23387182044131416  Accuracy :  0.8965178571428571 Time  0.1 s\nEpoch :  9  Batch :  1450  Loss :   0.23421295749730078  Accuracy :  0.8963793103448275 Time  0.09 s\nEpoch :  9  Batch :  1500  Loss :   0.23431150472164153  Accuracy :  0.89625 Time  0.09 s\nEpoch :  9  Batch :  1550  Loss :   0.2341981879934188  Accuracy :  0.8962903225806451 Time  0.09 s\nEpoch :  9  Batch :  1600  Loss :   0.2343887849431485  Accuracy :  0.8965234375 Time  0.09 s\nEpoch :  9  Batch :  1650  Loss :   0.23369687097542213  Accuracy :  0.8968181818181818 Time  0.09 s\nEpoch :  9  Batch :  1700  Loss :   0.2349851993515211  Accuracy :  0.8965441176470588 Time  0.1 s\nEpoch :  9  Batch :  1750  Loss :   0.23535932943650653  Accuracy :  0.8966785714285714 Time  0.09 s\nEpoch :  9  Batch :  1800  Loss :   0.2358679729948441  Accuracy :  0.8964236111111111 Time  0.09 s\nEpoch :  9  Batch :  1850  Loss :   0.2354904524619515  Accuracy :  0.8964189189189189 Time  0.09 s\nAccuracy of     0 : 57 %\nAccuracy of     1 : 63 %\nAccuracy of     2 : 91 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 97 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 97 %\nAccuracy of     7 : 96 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 94 %\nAccuracy of    10 : 97 %\nAccuracy of    11 : 97 %\nAccuracy of    12 : 89 %\nAccuracy of    13 : 94 %\nAccuracy of    14 : 97 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 98 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 94 %\nAccuracy of    21 : 100 %\nAccuracy of    22 : 97 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 39 %\nAccuracy of    25 : 86 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 70 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 60 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 82 %\nAccuracy of    34 : 98 %\nAccuracy of    35 : 84 %\nAccuracy of    36 : 96 %\nAccuracy of    37 : 86 %\nAccuracy of    38 : 93 %\nAccuracy of    39 : 87 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 96 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 97 %\nAccuracy of    45 : 97 %\nAccuracy of    46 : 95 %\nAccuracy of    47 : 65 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 41 %\nAccuracy of    51 : 84 %\nAccuracy of    52 : 97 %\nAccuracy of    53 : 97 %\nAccuracy of    54 : 66 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 58 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 78 %\nAccuracy of    60 : 96 %\nAccuracy of    61 : 72 %\n[9 epoch] Accuracy of the network on the Training images: 89 %\nEpoch :  10  Batch :  50  Loss :   0.21190836131572724  Accuracy :  0.90125 Time  0.09 s\nEpoch :  10  Batch :  100  Loss :   0.2317862007021904  Accuracy :  0.8975 Time  0.09 s\nEpoch :  10  Batch :  150  Loss :   0.2354427299896876  Accuracy :  0.8933333333333333 Time  0.09 s\nEpoch :  10  Batch :  200  Loss :   0.22373825073242187  Accuracy :  0.8990625 Time  0.09 s\nEpoch :  10  Batch :  250  Loss :   0.21881734347343446  Accuracy :  0.90175 Time  0.1 s\nEpoch :  10  Batch :  300  Loss :   0.21788324842850368  Accuracy :  0.9027083333333333 Time  0.09 s\nEpoch :  10  Batch :  350  Loss :   0.21987521695239204  Accuracy :  0.9026785714285714 Time  0.09 s\nEpoch :  10  Batch :  400  Loss :   0.22463160555809736  Accuracy :  0.89984375 Time  0.1 s\nEpoch :  10  Batch :  450  Loss :   0.22512256301111644  Accuracy :  0.8994444444444445 Time  0.09 s\nEpoch :  10  Batch :  500  Loss :   0.22960491320490836  Accuracy :  0.897625 Time  0.09 s\nEpoch :  10  Batch :  550  Loss :   0.23036997201767834  Accuracy :  0.8967045454545455 Time  0.09 s\nEpoch :  10  Batch :  600  Loss :   0.22701401216288408  Accuracy :  0.8988541666666666 Time  0.09 s\nEpoch :  10  Batch :  650  Loss :   0.22643709927797318  Accuracy :  0.8992307692307693 Time  0.09 s\nEpoch :  10  Batch :  700  Loss :   0.22685390819396292  Accuracy :  0.899375 Time  0.1 s\nEpoch :  10  Batch :  750  Loss :   0.22982081693410875  Accuracy :  0.899 Time  0.09 s\nEpoch :  10  Batch :  800  Loss :   0.22884932009503245  Accuracy :  0.89890625 Time  0.11 s\nEpoch :  10  Batch :  850  Loss :   0.2322546837435049  Accuracy :  0.8977941176470589 Time  0.09 s\nEpoch :  10  Batch :  900  Loss :   0.23109613021214803  Accuracy :  0.8980555555555556 Time  0.09 s\nEpoch :  10  Batch :  950  Loss :   0.22935711882616344  Accuracy :  0.89875 Time  0.09 s\nEpoch :  10  Batch :  1000  Loss :   0.22942460262775421  Accuracy :  0.8984375 Time  0.09 s\nEpoch :  10  Batch :  1050  Loss :   0.2303412184544972  Accuracy :  0.8976785714285714 Time  0.09 s\nEpoch :  10  Batch :  1100  Loss :   0.22889823897318406  Accuracy :  0.8982386363636363 Time  0.11 s\nEpoch :  10  Batch :  1150  Loss :   0.2291006559651831  Accuracy :  0.8980434782608696 Time  0.09 s\nEpoch :  10  Batch :  1200  Loss :   0.2278448470433553  Accuracy :  0.8984375 Time  0.09 s\nEpoch :  10  Batch :  1250  Loss :   0.2286203311443329  Accuracy :  0.89845 Time  0.11 s\nEpoch :  10  Batch :  1300  Loss :   0.22857194455770347  Accuracy :  0.8985096153846154 Time  0.09 s\nEpoch :  10  Batch :  1350  Loss :   0.22812655764597434  Accuracy :  0.8992592592592593 Time  0.09 s\nEpoch :  10  Batch :  1400  Loss :   0.22836243297372547  Accuracy :  0.899375 Time  0.1 s\nEpoch :  10  Batch :  1450  Loss :   0.2283094443946049  Accuracy :  0.899353448275862 Time  0.09 s\nEpoch :  10  Batch :  1500  Loss :   0.22682403955856958  Accuracy :  0.900125 Time  0.1 s\nEpoch :  10  Batch :  1550  Loss :   0.22731205025026874  Accuracy :  0.9001612903225806 Time  0.1 s\nEpoch :  10  Batch :  1600  Loss :   0.2278101280517876  Accuracy :  0.9002734375 Time  0.09 s\nEpoch :  10  Batch :  1650  Loss :   0.2279845736424128  Accuracy :  0.9003030303030303 Time  0.09 s\nEpoch :  10  Batch :  1700  Loss :   0.22904818455086035  Accuracy :  0.9004044117647059 Time  0.09 s\nEpoch :  10  Batch :  1750  Loss :   0.22937788931812558  Accuracy :  0.9005714285714286 Time  0.09 s\nEpoch :  10  Batch :  1800  Loss :   0.22992495622899797  Accuracy :  0.9000347222222222 Time  0.1 s\nEpoch :  10  Batch :  1850  Loss :   0.22986448383009112  Accuracy :  0.9001351351351351 Time  0.09 s\nAccuracy of     0 : 53 %\nAccuracy of     1 : 64 %\nAccuracy of     2 : 93 %\nAccuracy of     3 : 98 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 97 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 98 %\nAccuracy of     9 : 95 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 99 %\nAccuracy of    12 : 88 %\nAccuracy of    13 : 94 %\nAccuracy of    14 : 96 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 99 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 94 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 38 %\nAccuracy of    25 : 85 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 71 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 63 %\nAccuracy of    32 : 98 %\nAccuracy of    33 : 84 %\nAccuracy of    34 : 99 %\nAccuracy of    35 : 87 %\nAccuracy of    36 : 98 %\nAccuracy of    37 : 85 %\nAccuracy of    38 : 94 %\nAccuracy of    39 : 87 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 97 %\nAccuracy of    42 : 96 %\nAccuracy of    43 : 98 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 95 %\nAccuracy of    47 : 63 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 97 %\nAccuracy of    50 : 46 %\nAccuracy of    51 : 81 %\nAccuracy of    52 : 95 %\nAccuracy of    53 : 99 %\nAccuracy of    54 : 65 %\nAccuracy of    55 : 98 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 65 %\nAccuracy of    58 : 97 %\nAccuracy of    59 : 79 %\nAccuracy of    60 : 97 %\nAccuracy of    61 : 74 %\n[10 epoch] Accuracy of the network on the Training images: 90 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  11  Batch :  50  Loss :   0.22424812614917755  Accuracy :  0.89875 Time  0.09 s\nEpoch :  11  Batch :  100  Loss :   0.23349096417427062  Accuracy :  0.8975 Time  0.09 s\nEpoch :  11  Batch :  150  Loss :   0.22836672981580097  Accuracy :  0.8991666666666667 Time  0.09 s\nEpoch :  11  Batch :  200  Loss :   0.22132426127791405  Accuracy :  0.901875 Time  0.09 s\nEpoch :  11  Batch :  250  Loss :   0.22097621834278106  Accuracy :  0.9025 Time  0.09 s\nEpoch :  11  Batch :  300  Loss :   0.2214740613102913  Accuracy :  0.901875 Time  0.09 s\nEpoch :  11  Batch :  350  Loss :   0.2211172414251736  Accuracy :  0.9019642857142857 Time  0.09 s\nEpoch :  11  Batch :  400  Loss :   0.21845412272959946  Accuracy :  0.90421875 Time  0.09 s\nEpoch :  11  Batch :  450  Loss :   0.2148123546441396  Accuracy :  0.9066666666666666 Time  0.09 s\nEpoch :  11  Batch :  500  Loss :   0.21623283153772355  Accuracy :  0.905625 Time  0.09 s\nEpoch :  11  Batch :  550  Loss :   0.21864745616912842  Accuracy :  0.905 Time  0.09 s\nEpoch :  11  Batch :  600  Loss :   0.2174370899051428  Accuracy :  0.9052083333333333 Time  0.09 s\nEpoch :  11  Batch :  650  Loss :   0.2172544784958546  Accuracy :  0.9051923076923077 Time  0.09 s\nEpoch :  11  Batch :  700  Loss :   0.2182502097955772  Accuracy :  0.905 Time  0.09 s\nEpoch :  11  Batch :  750  Loss :   0.21734237788120905  Accuracy :  0.9051666666666667 Time  0.09 s\nEpoch :  11  Batch :  800  Loss :   0.21630810217931867  Accuracy :  0.90609375 Time  0.09 s\nEpoch :  11  Batch :  850  Loss :   0.21517821194494471  Accuracy :  0.90625 Time  0.09 s\nEpoch :  11  Batch :  900  Loss :   0.21773296546604898  Accuracy :  0.905 Time  0.09 s\nEpoch :  11  Batch :  950  Loss :   0.21716893801563664  Accuracy :  0.904671052631579 Time  0.09 s\nEpoch :  11  Batch :  1000  Loss :   0.21923699221014978  Accuracy :  0.9036875 Time  0.09 s\nEpoch :  11  Batch :  1050  Loss :   0.21790388884998504  Accuracy :  0.9042261904761905 Time  0.09 s\nEpoch :  11  Batch :  1100  Loss :   0.21917291928421367  Accuracy :  0.9032954545454546 Time  0.09 s\nEpoch :  11  Batch :  1150  Loss :   0.21780486241630886  Accuracy :  0.903695652173913 Time  0.1 s\nEpoch :  11  Batch :  1200  Loss :   0.21820844744642576  Accuracy :  0.9038541666666666 Time  0.09 s\nEpoch :  11  Batch :  1250  Loss :   0.21813329305648804  Accuracy :  0.90425 Time  0.09 s\nEpoch :  11  Batch :  1300  Loss :   0.21892298728227616  Accuracy :  0.9037980769230769 Time  0.09 s\nEpoch :  11  Batch :  1350  Loss :   0.21981583559954607  Accuracy :  0.9036111111111111 Time  0.09 s\nEpoch :  11  Batch :  1400  Loss :   0.22005977639130184  Accuracy :  0.9032142857142857 Time  0.09 s\nEpoch :  11  Batch :  1450  Loss :   0.21883133111328915  Accuracy :  0.9038793103448276 Time  0.09 s\nEpoch :  11  Batch :  1500  Loss :   0.21876588807503383  Accuracy :  0.9039583333333333 Time  0.09 s\nEpoch :  11  Batch :  1550  Loss :   0.21919000118009505  Accuracy :  0.9038709677419355 Time  0.09 s\nEpoch :  11  Batch :  1600  Loss :   0.21961536107584834  Accuracy :  0.90359375 Time  0.09 s\nEpoch :  11  Batch :  1650  Loss :   0.22127529007918906  Accuracy :  0.9032575757575757 Time  0.09 s\nEpoch :  11  Batch :  1700  Loss :   0.22078872784095652  Accuracy :  0.9036764705882353 Time  0.09 s\nEpoch :  11  Batch :  1750  Loss :   0.2215194799559457  Accuracy :  0.9034285714285715 Time  0.09 s\nEpoch :  11  Batch :  1800  Loss :   0.222280040913158  Accuracy :  0.9029513888888889 Time  0.09 s\nEpoch :  11  Batch :  1850  Loss :   0.2229903856805853  Accuracy :  0.9026013513513513 Time  0.09 s\nAccuracy of     0 : 58 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 92 %\nAccuracy of     3 : 98 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 99 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 95 %\nAccuracy of    10 : 97 %\nAccuracy of    11 : 97 %\nAccuracy of    12 : 89 %\nAccuracy of    13 : 95 %\nAccuracy of    14 : 98 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 98 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 98 %\nAccuracy of    20 : 95 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 41 %\nAccuracy of    25 : 85 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 74 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 98 %\nAccuracy of    31 : 62 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 82 %\nAccuracy of    34 : 97 %\nAccuracy of    35 : 85 %\nAccuracy of    36 : 96 %\nAccuracy of    37 : 86 %\nAccuracy of    38 : 93 %\nAccuracy of    39 : 88 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 96 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 96 %\nAccuracy of    47 : 62 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 43 %\nAccuracy of    51 : 85 %\nAccuracy of    52 : 97 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 70 %\nAccuracy of    55 : 98 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 66 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 78 %\nAccuracy of    60 : 96 %\nAccuracy of    61 : 74 %\n[11 epoch] Accuracy of the network on the Training images: 90 %\nEpoch :  12  Batch :  50  Loss :   0.2205465567111969  Accuracy :  0.90125 Time  0.09 s\nEpoch :  12  Batch :  100  Loss :   0.20906239479780198  Accuracy :  0.90375 Time  0.09 s\nEpoch :  12  Batch :  150  Loss :   0.2222046532233556  Accuracy :  0.9020833333333333 Time  0.09 s\nEpoch :  12  Batch :  200  Loss :   0.22264840096235275  Accuracy :  0.9025 Time  0.09 s\nEpoch :  12  Batch :  250  Loss :   0.2146723368167877  Accuracy :  0.9055 Time  0.09 s\nEpoch :  12  Batch :  300  Loss :   0.21544142345587414  Accuracy :  0.9064583333333334 Time  0.09 s\nEpoch :  12  Batch :  350  Loss :   0.21507462969848087  Accuracy :  0.9058928571428572 Time  0.09 s\nEpoch :  12  Batch :  400  Loss :   0.21284906439483164  Accuracy :  0.90734375 Time  0.09 s\nEpoch :  12  Batch :  450  Loss :   0.2128351338704427  Accuracy :  0.9077777777777778 Time  0.09 s\nEpoch :  12  Batch :  500  Loss :   0.21497867560386658  Accuracy :  0.907125 Time  0.09 s\nEpoch :  12  Batch :  550  Loss :   0.21176108544523065  Accuracy :  0.9082954545454546 Time  0.09 s\nEpoch :  12  Batch :  600  Loss :   0.21054112364848454  Accuracy :  0.9091666666666667 Time  0.09 s\nEpoch :  12  Batch :  650  Loss :   0.21355904407226123  Accuracy :  0.9082692307692307 Time  0.1 s\nEpoch :  12  Batch :  700  Loss :   0.21497759037784167  Accuracy :  0.9070535714285715 Time  0.09 s\nEpoch :  12  Batch :  750  Loss :   0.2139207044839859  Accuracy :  0.9083333333333333 Time  0.09 s\nEpoch :  12  Batch :  800  Loss :   0.21450991071760656  Accuracy :  0.907734375 Time  0.09 s\nEpoch :  12  Batch :  850  Loss :   0.21393397250596216  Accuracy :  0.9080147058823529 Time  0.1 s\nEpoch :  12  Batch :  900  Loss :   0.2141469860739178  Accuracy :  0.9076388888888889 Time  0.09 s\nEpoch :  12  Batch :  950  Loss :   0.2146237765644726  Accuracy :  0.9076973684210526 Time  0.1 s\nEpoch :  12  Batch :  1000  Loss :   0.2143911175876856  Accuracy :  0.9070625 Time  0.09 s\nEpoch :  12  Batch :  1050  Loss :   0.21474788073982512  Accuracy :  0.9064880952380953 Time  0.09 s\nEpoch :  12  Batch :  1100  Loss :   0.21321666144511917  Accuracy :  0.9068181818181819 Time  0.09 s\nEpoch :  12  Batch :  1150  Loss :   0.2116927986689236  Accuracy :  0.9073369565217392 Time  0.09 s\nEpoch :  12  Batch :  1200  Loss :   0.213095279323558  Accuracy :  0.90703125 Time  0.09 s\nEpoch :  12  Batch :  1250  Loss :   0.21326040230989457  Accuracy :  0.9069 Time  0.09 s\nEpoch :  12  Batch :  1300  Loss :   0.21337225837203172  Accuracy :  0.9067307692307692 Time  0.09 s\nEpoch :  12  Batch :  1350  Loss :   0.21293797522783278  Accuracy :  0.9070370370370371 Time  0.09 s\nEpoch :  12  Batch :  1400  Loss :   0.21168113715946674  Accuracy :  0.9073214285714286 Time  0.09 s\nEpoch :  12  Batch :  1450  Loss :   0.21082031670315513  Accuracy :  0.9075431034482758 Time  0.09 s\nEpoch :  12  Batch :  1500  Loss :   0.21178545001149177  Accuracy :  0.90675 Time  0.09 s\nEpoch :  12  Batch :  1550  Loss :   0.2120225923195962  Accuracy :  0.9062903225806451 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  12  Batch :  1600  Loss :   0.21226401922293006  Accuracy :  0.9060546875 Time  0.09 s\nEpoch :  12  Batch :  1650  Loss :   0.21248338133096695  Accuracy :  0.905719696969697 Time  0.09 s\nEpoch :  12  Batch :  1700  Loss :   0.21222948729991914  Accuracy :  0.9061397058823529 Time  0.09 s\nEpoch :  12  Batch :  1750  Loss :   0.21222469404765537  Accuracy :  0.906 Time  0.09 s\nEpoch :  12  Batch :  1800  Loss :   0.21308057616154352  Accuracy :  0.9058333333333334 Time  0.09 s\nEpoch :  12  Batch :  1850  Loss :   0.21354528470619305  Accuracy :  0.9053716216216217 Time  0.09 s\nAccuracy of     0 : 57 %\nAccuracy of     1 : 61 %\nAccuracy of     2 : 95 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 98 %\nAccuracy of     9 : 96 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 98 %\nAccuracy of    12 : 90 %\nAccuracy of    13 : 95 %\nAccuracy of    14 : 97 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 98 %\nAccuracy of    17 : 97 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 98 %\nAccuracy of    20 : 95 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 43 %\nAccuracy of    25 : 86 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 74 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 65 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 82 %\nAccuracy of    34 : 98 %\nAccuracy of    35 : 86 %\nAccuracy of    36 : 96 %\nAccuracy of    37 : 88 %\nAccuracy of    38 : 94 %\nAccuracy of    39 : 90 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 98 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 97 %\nAccuracy of    47 : 62 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 45 %\nAccuracy of    51 : 85 %\nAccuracy of    52 : 97 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 69 %\nAccuracy of    55 : 98 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 65 %\nAccuracy of    58 : 97 %\nAccuracy of    59 : 80 %\nAccuracy of    60 : 94 %\nAccuracy of    61 : 77 %\n[12 epoch] Accuracy of the network on the Training images: 90 %\nEpoch :  13  Batch :  50  Loss :   0.18407652914524078  Accuracy :  0.91875 Time  0.09 s\nEpoch :  13  Batch :  100  Loss :   0.185872206389904  Accuracy :  0.91 Time  0.09 s\nEpoch :  13  Batch :  150  Loss :   0.1891407750050227  Accuracy :  0.9108333333333334 Time  0.09 s\nEpoch :  13  Batch :  200  Loss :   0.1823936741054058  Accuracy :  0.914375 Time  0.09 s\nEpoch :  13  Batch :  250  Loss :   0.1808756204843521  Accuracy :  0.9145 Time  0.09 s\nEpoch :  13  Batch :  300  Loss :   0.17946850270032882  Accuracy :  0.91625 Time  0.09 s\nEpoch :  13  Batch :  350  Loss :   0.18594923274857658  Accuracy :  0.9126785714285715 Time  0.09 s\nEpoch :  13  Batch :  400  Loss :   0.18531828798353672  Accuracy :  0.915 Time  0.09 s\nEpoch :  13  Batch :  450  Loss :   0.1917162196834882  Accuracy :  0.9116666666666666 Time  0.09 s\nEpoch :  13  Batch :  500  Loss :   0.19728456905484199  Accuracy :  0.910375 Time  0.09 s\nEpoch :  13  Batch :  550  Loss :   0.1956812131675807  Accuracy :  0.9117045454545455 Time  0.09 s\nEpoch :  13  Batch :  600  Loss :   0.19709088707963626  Accuracy :  0.9117708333333333 Time  0.09 s\nEpoch :  13  Batch :  650  Loss :   0.1993534056498454  Accuracy :  0.910673076923077 Time  0.09 s\nEpoch :  13  Batch :  700  Loss :   0.19826067409345083  Accuracy :  0.9108035714285714 Time  0.09 s\nEpoch :  13  Batch :  750  Loss :   0.19764822590351105  Accuracy :  0.911 Time  0.09 s\nEpoch :  13  Batch :  800  Loss :   0.20054263163357974  Accuracy :  0.90921875 Time  0.09 s\nEpoch :  13  Batch :  850  Loss :   0.20164248024716097  Accuracy :  0.9083088235294118 Time  0.09 s\nEpoch :  13  Batch :  900  Loss :   0.2016999872525533  Accuracy :  0.9084722222222222 Time  0.09 s\nEpoch :  13  Batch :  950  Loss :   0.20297356795323523  Accuracy :  0.9077631578947368 Time  0.09 s\nEpoch :  13  Batch :  1000  Loss :   0.20360304440557958  Accuracy :  0.90775 Time  0.11 s\nEpoch :  13  Batch :  1050  Loss :   0.20449363145090285  Accuracy :  0.9069047619047619 Time  0.09 s\nEpoch :  13  Batch :  1100  Loss :   0.20562015636400743  Accuracy :  0.9066477272727272 Time  0.09 s\nEpoch :  13  Batch :  1150  Loss :   0.20721133530139924  Accuracy :  0.9063043478260869 Time  0.09 s\nEpoch :  13  Batch :  1200  Loss :   0.20750889737159015  Accuracy :  0.9063541666666667 Time  0.09 s\nEpoch :  13  Batch :  1250  Loss :   0.20810193330049515  Accuracy :  0.9063 Time  0.09 s\nEpoch :  13  Batch :  1300  Loss :   0.208405188448154  Accuracy :  0.906298076923077 Time  0.09 s\nEpoch :  13  Batch :  1350  Loss :   0.2082659406684063  Accuracy :  0.9061111111111111 Time  0.09 s\nEpoch :  13  Batch :  1400  Loss :   0.20892462373844214  Accuracy :  0.9057142857142857 Time  0.09 s\nEpoch :  13  Batch :  1450  Loss :   0.21029254032620068  Accuracy :  0.9048706896551724 Time  0.09 s\nEpoch :  13  Batch :  1500  Loss :   0.20999438648422558  Accuracy :  0.9049583333333333 Time  0.1 s\nEpoch :  13  Batch :  1550  Loss :   0.21059830020512305  Accuracy :  0.9046774193548387 Time  0.09 s\nEpoch :  13  Batch :  1600  Loss :   0.21049349507316947  Accuracy :  0.90484375 Time  0.09 s\nEpoch :  13  Batch :  1650  Loss :   0.21050238605701563  Accuracy :  0.9049242424242424 Time  0.09 s\nEpoch :  13  Batch :  1700  Loss :   0.20982975584619185  Accuracy :  0.9054044117647059 Time  0.1 s\nEpoch :  13  Batch :  1750  Loss :   0.21042017141410282  Accuracy :  0.9052857142857142 Time  0.09 s\nEpoch :  13  Batch :  1800  Loss :   0.2094750700228744  Accuracy :  0.9059027777777777 Time  0.09 s\nEpoch :  13  Batch :  1850  Loss :   0.2094544723388311  Accuracy :  0.9059459459459459 Time  0.09 s\nAccuracy of     0 : 58 %\nAccuracy of     1 : 63 %\nAccuracy of     2 : 93 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 99 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 98 %\nAccuracy of     7 : 99 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 97 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 97 %\nAccuracy of    12 : 90 %\nAccuracy of    13 : 95 %\nAccuracy of    14 : 97 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 98 %\nAccuracy of    17 : 99 %\nAccuracy of    18 : 98 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 95 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 42 %\nAccuracy of    25 : 84 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 98 %\nAccuracy of    28 : 72 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 96 %\nAccuracy of    31 : 67 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 83 %\nAccuracy of    34 : 98 %\nAccuracy of    35 : 88 %\nAccuracy of    36 : 97 %\nAccuracy of    37 : 88 %\nAccuracy of    38 : 93 %\nAccuracy of    39 : 89 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 97 %\nAccuracy of    42 : 98 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 96 %\nAccuracy of    47 : 64 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 99 %\nAccuracy of    50 : 49 %\nAccuracy of    51 : 83 %\nAccuracy of    52 : 96 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 66 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 66 %\nAccuracy of    58 : 97 %\nAccuracy of    59 : 79 %\nAccuracy of    60 : 97 %\nAccuracy of    61 : 78 %\n[13 epoch] Accuracy of the network on the Training images: 90 %\nEpoch :  14  Batch :  50  Loss :   0.20198893427848816  Accuracy :  0.92125 Time  0.09 s\nEpoch :  14  Batch :  100  Loss :   0.19888451665639878  Accuracy :  0.9225 Time  0.1 s\nEpoch :  14  Batch :  150  Loss :   0.19657548666000366  Accuracy :  0.91875 Time  0.09 s\nEpoch :  14  Batch :  200  Loss :   0.19320226475596428  Accuracy :  0.9190625 Time  0.09 s\nEpoch :  14  Batch :  250  Loss :   0.19579047048091888  Accuracy :  0.91775 Time  0.09 s\nEpoch :  14  Batch :  300  Loss :   0.19744401757915814  Accuracy :  0.9160416666666666 Time  0.09 s\nEpoch :  14  Batch :  350  Loss :   0.19662170559167863  Accuracy :  0.9176785714285715 Time  0.09 s\nEpoch :  14  Batch :  400  Loss :   0.19434065226465463  Accuracy :  0.91796875 Time  0.09 s\nEpoch :  14  Batch :  450  Loss :   0.19529871301518545  Accuracy :  0.9176388888888889 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  14  Batch :  500  Loss :   0.19600188556313514  Accuracy :  0.917 Time  0.09 s\nEpoch :  14  Batch :  550  Loss :   0.19626029141924597  Accuracy :  0.9164772727272728 Time  0.09 s\nEpoch :  14  Batch :  600  Loss :   0.19982582482198874  Accuracy :  0.915 Time  0.09 s\nEpoch :  14  Batch :  650  Loss :   0.20060464682487342  Accuracy :  0.9142307692307692 Time  0.09 s\nEpoch :  14  Batch :  700  Loss :   0.20036204199705804  Accuracy :  0.9141071428571429 Time  0.09 s\nEpoch :  14  Batch :  750  Loss :   0.2015761244893074  Accuracy :  0.9133333333333333 Time  0.09 s\nEpoch :  14  Batch :  800  Loss :   0.20008817179128527  Accuracy :  0.91453125 Time  0.09 s\nEpoch :  14  Batch :  850  Loss :   0.1995556676212479  Accuracy :  0.9143382352941176 Time  0.09 s\nEpoch :  14  Batch :  900  Loss :   0.19749762791726325  Accuracy :  0.9146527777777778 Time  0.09 s\nEpoch :  14  Batch :  950  Loss :   0.19741940484235163  Accuracy :  0.9145394736842105 Time  0.09 s\nEpoch :  14  Batch :  1000  Loss :   0.19805808086693286  Accuracy :  0.914125 Time  0.09 s\nEpoch :  14  Batch :  1050  Loss :   0.19956958193154561  Accuracy :  0.9132738095238095 Time  0.09 s\nEpoch :  14  Batch :  1100  Loss :   0.20029787411743943  Accuracy :  0.9125 Time  0.09 s\nEpoch :  14  Batch :  1150  Loss :   0.2000238692630892  Accuracy :  0.9123369565217392 Time  0.09 s\nEpoch :  14  Batch :  1200  Loss :   0.20007685642689466  Accuracy :  0.9118229166666667 Time  0.09 s\nEpoch :  14  Batch :  1250  Loss :   0.2006036751151085  Accuracy :  0.9112 Time  0.09 s\nEpoch :  14  Batch :  1300  Loss :   0.20069340177453482  Accuracy :  0.9109134615384615 Time  0.09 s\nEpoch :  14  Batch :  1350  Loss :   0.20267379073081193  Accuracy :  0.9103240740740741 Time  0.09 s\nEpoch :  14  Batch :  1400  Loss :   0.20191762678325176  Accuracy :  0.9108482142857143 Time  0.09 s\nEpoch :  14  Batch :  1450  Loss :   0.20165456620783642  Accuracy :  0.9108189655172414 Time  0.09 s\nEpoch :  14  Batch :  1500  Loss :   0.20178763740261396  Accuracy :  0.911 Time  0.09 s\nEpoch :  14  Batch :  1550  Loss :   0.2026872215444042  Accuracy :  0.9106451612903226 Time  0.09 s\nEpoch :  14  Batch :  1600  Loss :   0.2029894019756466  Accuracy :  0.9106640625 Time  0.09 s\nEpoch :  14  Batch :  1650  Loss :   0.2031312785636295  Accuracy :  0.9106439393939394 Time  0.09 s\nEpoch :  14  Batch :  1700  Loss :   0.20281395289827794  Accuracy :  0.9105514705882353 Time  0.11 s\nEpoch :  14  Batch :  1750  Loss :   0.20254162951878138  Accuracy :  0.9103928571428571 Time  0.09 s\nEpoch :  14  Batch :  1800  Loss :   0.20354631366001236  Accuracy :  0.9100347222222223 Time  0.09 s\nEpoch :  14  Batch :  1850  Loss :   0.20427757430720975  Accuracy :  0.9097972972972973 Time  0.09 s\nAccuracy of     0 : 62 %\nAccuracy of     1 : 65 %\nAccuracy of     2 : 93 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 97 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 98 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 98 %\nAccuracy of     9 : 96 %\nAccuracy of    10 : 99 %\nAccuracy of    11 : 98 %\nAccuracy of    12 : 89 %\nAccuracy of    13 : 96 %\nAccuracy of    14 : 97 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 99 %\nAccuracy of    17 : 97 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 96 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 43 %\nAccuracy of    25 : 86 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 72 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 98 %\nAccuracy of    31 : 64 %\nAccuracy of    32 : 98 %\nAccuracy of    33 : 83 %\nAccuracy of    34 : 99 %\nAccuracy of    35 : 89 %\nAccuracy of    36 : 98 %\nAccuracy of    37 : 87 %\nAccuracy of    38 : 94 %\nAccuracy of    39 : 88 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 97 %\nAccuracy of    42 : 98 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 99 %\nAccuracy of    46 : 97 %\nAccuracy of    47 : 63 %\nAccuracy of    48 : 97 %\nAccuracy of    49 : 97 %\nAccuracy of    50 : 48 %\nAccuracy of    51 : 85 %\nAccuracy of    52 : 96 %\nAccuracy of    53 : 99 %\nAccuracy of    54 : 69 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 99 %\nAccuracy of    57 : 65 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 82 %\nAccuracy of    60 : 98 %\nAccuracy of    61 : 75 %\n[14 epoch] Accuracy of the network on the Training images: 90 %\nEpoch :  15  Batch :  50  Loss :   0.20075659275054933  Accuracy :  0.9175 Time  0.09 s\nEpoch :  15  Batch :  100  Loss :   0.18695027202367784  Accuracy :  0.92 Time  0.09 s\nEpoch :  15  Batch :  150  Loss :   0.19862935105959575  Accuracy :  0.9141666666666667 Time  0.09 s\nEpoch :  15  Batch :  200  Loss :   0.1963629776239395  Accuracy :  0.9165625 Time  0.09 s\nEpoch :  15  Batch :  250  Loss :   0.20140152883529663  Accuracy :  0.914 Time  0.09 s\nEpoch :  15  Batch :  300  Loss :   0.20312990774710973  Accuracy :  0.91375 Time  0.09 s\nEpoch :  15  Batch :  350  Loss :   0.20523404828139713  Accuracy :  0.9135714285714286 Time  0.09 s\nEpoch :  15  Batch :  400  Loss :   0.20293671563267707  Accuracy :  0.91390625 Time  0.11 s\nEpoch :  15  Batch :  450  Loss :   0.20434616910086736  Accuracy :  0.9140277777777778 Time  0.09 s\nEpoch :  15  Batch :  500  Loss :   0.2034413932263851  Accuracy :  0.91425 Time  0.09 s\nEpoch :  15  Batch :  550  Loss :   0.20343467563390732  Accuracy :  0.9135227272727273 Time  0.09 s\nEpoch :  15  Batch :  600  Loss :   0.20105071095128854  Accuracy :  0.9139583333333333 Time  0.09 s\nEpoch :  15  Batch :  650  Loss :   0.20020258020896178  Accuracy :  0.9133653846153846 Time  0.09 s\nEpoch :  15  Batch :  700  Loss :   0.19933107706052916  Accuracy :  0.9138392857142857 Time  0.09 s\nEpoch :  15  Batch :  750  Loss :   0.19804190212488174  Accuracy :  0.9144166666666667 Time  0.09 s\nEpoch :  15  Batch :  800  Loss :   0.19841043720021845  Accuracy :  0.913515625 Time  0.09 s\nEpoch :  15  Batch :  850  Loss :   0.19862616824753143  Accuracy :  0.9134558823529412 Time  0.09 s\nEpoch :  15  Batch :  900  Loss :   0.20083894492851365  Accuracy :  0.9124305555555555 Time  0.09 s\nEpoch :  15  Batch :  950  Loss :   0.2004201925898853  Accuracy :  0.9130263157894737 Time  0.09 s\nEpoch :  15  Batch :  1000  Loss :   0.20010320772230625  Accuracy :  0.9133125 Time  0.09 s\nEpoch :  15  Batch :  1050  Loss :   0.2024602744550932  Accuracy :  0.9126785714285715 Time  0.09 s\nEpoch :  15  Batch :  1100  Loss :   0.20232496617869897  Accuracy :  0.9126136363636363 Time  0.09 s\nEpoch :  15  Batch :  1150  Loss :   0.20361966568490733  Accuracy :  0.9123369565217392 Time  0.09 s\nEpoch :  15  Batch :  1200  Loss :   0.20455885152022044  Accuracy :  0.9119270833333334 Time  0.09 s\nEpoch :  15  Batch :  1250  Loss :   0.2046375539302826  Accuracy :  0.91165 Time  0.09 s\nEpoch :  15  Batch :  1300  Loss :   0.20522372932388233  Accuracy :  0.9111538461538462 Time  0.09 s\nEpoch :  15  Batch :  1350  Loss :   0.2045539735975089  Accuracy :  0.91125 Time  0.09 s\nEpoch :  15  Batch :  1400  Loss :   0.20340569503605366  Accuracy :  0.9117410714285714 Time  0.09 s\nEpoch :  15  Batch :  1450  Loss :   0.20343456838665339  Accuracy :  0.9117241379310345 Time  0.09 s\nEpoch :  15  Batch :  1500  Loss :   0.20295210813482603  Accuracy :  0.9117083333333333 Time  0.09 s\nEpoch :  15  Batch :  1550  Loss :   0.2028879292261216  Accuracy :  0.9118951612903226 Time  0.09 s\nEpoch :  15  Batch :  1600  Loss :   0.2029115371685475  Accuracy :  0.9120703125 Time  0.09 s\nEpoch :  15  Batch :  1650  Loss :   0.20227477070960131  Accuracy :  0.9120075757575757 Time  0.09 s\nEpoch :  15  Batch :  1700  Loss :   0.202563586068504  Accuracy :  0.9119852941176471 Time  0.09 s\nEpoch :  15  Batch :  1750  Loss :   0.20198464316129686  Accuracy :  0.9121428571428571 Time  0.09 s\nEpoch :  15  Batch :  1800  Loss :   0.20158780328929424  Accuracy :  0.9122916666666666 Time  0.09 s\nEpoch :  15  Batch :  1850  Loss :   0.20185759206881393  Accuracy :  0.9122635135135135 Time  0.09 s\nAccuracy of     0 : 62 %\nAccuracy of     1 : 65 %\nAccuracy of     2 : 94 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 99 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 96 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 98 %\nAccuracy of     9 : 95 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 97 %\nAccuracy of    12 : 90 %\nAccuracy of    13 : 96 %\nAccuracy of    14 : 98 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 99 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 98 %\nAccuracy of    20 : 96 %\nAccuracy of    21 : 100 %\nAccuracy of    22 : 99 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 47 %\nAccuracy of    25 : 88 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 74 %\nAccuracy of    29 : 100 %\nAccuracy of    30 : 98 %\nAccuracy of    31 : 66 %\nAccuracy of    32 : 98 %\nAccuracy of    33 : 84 %\nAccuracy of    34 : 97 %\nAccuracy of    35 : 89 %\nAccuracy of    36 : 98 %\nAccuracy of    37 : 87 %\nAccuracy of    38 : 95 %\nAccuracy of    39 : 90 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 98 %\nAccuracy of    42 : 98 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 97 %\nAccuracy of    47 : 64 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 99 %\nAccuracy of    50 : 45 %\nAccuracy of    51 : 84 %\nAccuracy of    52 : 96 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 72 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 99 %\nAccuracy of    57 : 68 %\nAccuracy of    58 : 97 %\nAccuracy of    59 : 82 %\nAccuracy of    60 : 98 %\nAccuracy of    61 : 78 %\n[15 epoch] Accuracy of the network on the Training images: 91 %\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  16  Batch :  50  Loss :   0.1896299397945404  Accuracy :  0.9125 Time  0.09 s\nEpoch :  16  Batch :  100  Loss :   0.19364363104104995  Accuracy :  0.913125 Time  0.09 s\nEpoch :  16  Batch :  150  Loss :   0.18896676580111185  Accuracy :  0.915 Time  0.09 s\nEpoch :  16  Batch :  200  Loss :   0.18206215307116508  Accuracy :  0.918125 Time  0.09 s\nEpoch :  16  Batch :  250  Loss :   0.1817080888748169  Accuracy :  0.91775 Time  0.09 s\nEpoch :  16  Batch :  300  Loss :   0.1877403677503268  Accuracy :  0.9129166666666667 Time  0.09 s\nEpoch :  16  Batch :  350  Loss :   0.18676408657005855  Accuracy :  0.9123214285714286 Time  0.09 s\nEpoch :  16  Batch :  400  Loss :   0.1891061905771494  Accuracy :  0.9121875 Time  0.09 s\nEpoch :  16  Batch :  450  Loss :   0.18822301818264855  Accuracy :  0.9133333333333333 Time  0.09 s\nEpoch :  16  Batch :  500  Loss :   0.19209597778320311  Accuracy :  0.911125 Time  0.09 s\nEpoch :  16  Batch :  550  Loss :   0.1953109878843481  Accuracy :  0.9105681818181818 Time  0.09 s\nEpoch :  16  Batch :  600  Loss :   0.1960317949205637  Accuracy :  0.9113541666666667 Time  0.09 s\nEpoch :  16  Batch :  650  Loss :   0.19588668976838772  Accuracy :  0.9110576923076923 Time  0.09 s\nEpoch :  16  Batch :  700  Loss :   0.19800870608006205  Accuracy :  0.9098214285714286 Time  0.09 s\nEpoch :  16  Batch :  750  Loss :   0.19933222911755244  Accuracy :  0.9095833333333333 Time  0.09 s\nEpoch :  16  Batch :  800  Loss :   0.19631421824917197  Accuracy :  0.9109375 Time  0.09 s\nEpoch :  16  Batch :  850  Loss :   0.19631099877988592  Accuracy :  0.9108823529411765 Time  0.11 s\nEpoch :  16  Batch :  900  Loss :   0.19547085182534324  Accuracy :  0.9113194444444445 Time  0.1 s\nEpoch :  16  Batch :  950  Loss :   0.1962640645002064  Accuracy :  0.9110526315789473 Time  0.09 s\nEpoch :  16  Batch :  1000  Loss :   0.1953261964172125  Accuracy :  0.91225 Time  0.09 s\nEpoch :  16  Batch :  1050  Loss :   0.19498768461602076  Accuracy :  0.9125 Time  0.09 s\nEpoch :  16  Batch :  1100  Loss :   0.1944926439496604  Accuracy :  0.9126136363636363 Time  0.09 s\nEpoch :  16  Batch :  1150  Loss :   0.19359655609597332  Accuracy :  0.9132065217391304 Time  0.09 s\nEpoch :  16  Batch :  1200  Loss :   0.19393133271485566  Accuracy :  0.9127604166666666 Time  0.09 s\nEpoch :  16  Batch :  1250  Loss :   0.19432635046243668  Accuracy :  0.91225 Time  0.09 s\nEpoch :  16  Batch :  1300  Loss :   0.19407872538153942  Accuracy :  0.9123076923076923 Time  0.09 s\nEpoch :  16  Batch :  1350  Loss :   0.19554215052613505  Accuracy :  0.9119907407407407 Time  0.09 s\nEpoch :  16  Batch :  1400  Loss :   0.19481665463319847  Accuracy :  0.9123660714285714 Time  0.09 s\nEpoch :  16  Batch :  1450  Loss :   0.19409248445568414  Accuracy :  0.9130172413793104 Time  0.09 s\nEpoch :  16  Batch :  1500  Loss :   0.19513119289278985  Accuracy :  0.9125833333333333 Time  0.09 s\nEpoch :  16  Batch :  1550  Loss :   0.19555315818517438  Accuracy :  0.9123790322580645 Time  0.09 s\nEpoch :  16  Batch :  1600  Loss :   0.1944540219474584  Accuracy :  0.9127734375 Time  0.09 s\nEpoch :  16  Batch :  1650  Loss :   0.19471377814357932  Accuracy :  0.9128030303030303 Time  0.09 s\nEpoch :  16  Batch :  1700  Loss :   0.1951954602756921  Accuracy :  0.9127573529411764 Time  0.12 s\nEpoch :  16  Batch :  1750  Loss :   0.19526307622875486  Accuracy :  0.9127857142857143 Time  0.1 s\nEpoch :  16  Batch :  1800  Loss :   0.1951601724574963  Accuracy :  0.9130208333333333 Time  0.09 s\nEpoch :  16  Batch :  1850  Loss :   0.1952543715447993  Accuracy :  0.912972972972973 Time  0.09 s\nAccuracy of     0 : 62 %\nAccuracy of     1 : 64 %\nAccuracy of     2 : 94 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 97 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 97 %\nAccuracy of    10 : 99 %\nAccuracy of    11 : 99 %\nAccuracy of    12 : 89 %\nAccuracy of    13 : 96 %\nAccuracy of    14 : 98 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 98 %\nAccuracy of    17 : 99 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 95 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 47 %\nAccuracy of    25 : 87 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 76 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 98 %\nAccuracy of    31 : 67 %\nAccuracy of    32 : 98 %\nAccuracy of    33 : 85 %\nAccuracy of    34 : 99 %\nAccuracy of    35 : 87 %\nAccuracy of    36 : 98 %\nAccuracy of    37 : 89 %\nAccuracy of    38 : 94 %\nAccuracy of    39 : 92 %\nAccuracy of    40 : 98 %\nAccuracy of    41 : 97 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 98 %\nAccuracy of    46 : 96 %\nAccuracy of    47 : 65 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 43 %\nAccuracy of    51 : 85 %\nAccuracy of    52 : 98 %\nAccuracy of    53 : 99 %\nAccuracy of    54 : 72 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 68 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 80 %\nAccuracy of    60 : 96 %\nAccuracy of    61 : 77 %\n[16 epoch] Accuracy of the network on the Training images: 91 %\nEpoch :  17  Batch :  50  Loss :   0.21168583750724793  Accuracy :  0.90875 Time  0.09 s\nEpoch :  17  Batch :  100  Loss :   0.21386083990335464  Accuracy :  0.90125 Time  0.09 s\nEpoch :  17  Batch :  150  Loss :   0.2025352934996287  Accuracy :  0.9070833333333334 Time  0.09 s\nEpoch :  17  Batch :  200  Loss :   0.19919225364923476  Accuracy :  0.90875 Time  0.09 s\nEpoch :  17  Batch :  250  Loss :   0.20008239483833312  Accuracy :  0.91125 Time  0.09 s\nEpoch :  17  Batch :  300  Loss :   0.20010197619597117  Accuracy :  0.9095833333333333 Time  0.1 s\nEpoch :  17  Batch :  350  Loss :   0.2040866964203971  Accuracy :  0.9082142857142858 Time  0.09 s\nEpoch :  17  Batch :  400  Loss :   0.2055367872864008  Accuracy :  0.90671875 Time  0.09 s\nEpoch :  17  Batch :  450  Loss :   0.20367662257618374  Accuracy :  0.9073611111111111 Time  0.09 s\nEpoch :  17  Batch :  500  Loss :   0.2071595823764801  Accuracy :  0.907875 Time  0.09 s\nEpoch :  17  Batch :  550  Loss :   0.20204879012974825  Accuracy :  0.9097727272727273 Time  0.09 s\nEpoch :  17  Batch :  600  Loss :   0.20162496080001197  Accuracy :  0.9096875 Time  0.09 s\nEpoch :  17  Batch :  650  Loss :   0.2008421149620643  Accuracy :  0.9095192307692308 Time  0.12 s\nEpoch :  17  Batch :  700  Loss :   0.1990974600825991  Accuracy :  0.9103571428571429 Time  0.1 s\nEpoch :  17  Batch :  750  Loss :   0.19786598998308183  Accuracy :  0.9113333333333333 Time  0.09 s\nEpoch :  17  Batch :  800  Loss :   0.19562118830159306  Accuracy :  0.912109375 Time  0.09 s\nEpoch :  17  Batch :  850  Loss :   0.19664883177070056  Accuracy :  0.9115441176470588 Time  0.09 s\nEpoch :  17  Batch :  900  Loss :   0.19494403238097827  Accuracy :  0.9115972222222222 Time  0.09 s\nEpoch :  17  Batch :  950  Loss :   0.19243220509667144  Accuracy :  0.9128947368421053 Time  0.09 s\nEpoch :  17  Batch :  1000  Loss :   0.1918828852623701  Accuracy :  0.91325 Time  0.09 s\nEpoch :  17  Batch :  1050  Loss :   0.19189299601884116  Accuracy :  0.9133928571428571 Time  0.09 s\nEpoch :  17  Batch :  1100  Loss :   0.19192435769872232  Accuracy :  0.9136931818181818 Time  0.12 s\nEpoch :  17  Batch :  1150  Loss :   0.19157142407220343  Accuracy :  0.9138586956521739 Time  0.09 s\nEpoch :  17  Batch :  1200  Loss :   0.19117329183965923  Accuracy :  0.9141145833333333 Time  0.09 s\nEpoch :  17  Batch :  1250  Loss :   0.19133670185804366  Accuracy :  0.91395 Time  0.09 s\nEpoch :  17  Batch :  1300  Loss :   0.19260002485834635  Accuracy :  0.9133173076923077 Time  0.09 s\nEpoch :  17  Batch :  1350  Loss :   0.19227899027091486  Accuracy :  0.91375 Time  0.09 s\nEpoch :  17  Batch :  1400  Loss :   0.19212472893297672  Accuracy :  0.9139732142857143 Time  0.09 s\nEpoch :  17  Batch :  1450  Loss :   0.19135191211412692  Accuracy :  0.9143965517241379 Time  0.09 s\nEpoch :  17  Batch :  1500  Loss :   0.19148342381914457  Accuracy :  0.9141666666666667 Time  0.13 s\nEpoch :  17  Batch :  1550  Loss :   0.19238309461262917  Accuracy :  0.9136290322580645 Time  0.09 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  17  Batch :  1600  Loss :   0.1928424434643239  Accuracy :  0.9133984375 Time  0.09 s\nEpoch :  17  Batch :  1650  Loss :   0.1932593091599869  Accuracy :  0.9133712121212121 Time  0.1 s\nEpoch :  17  Batch :  1700  Loss :   0.19309556188828805  Accuracy :  0.9133088235294118 Time  0.09 s\nEpoch :  17  Batch :  1750  Loss :   0.19302839932271412  Accuracy :  0.9131785714285714 Time  0.09 s\nEpoch :  17  Batch :  1800  Loss :   0.19284241187075774  Accuracy :  0.9134027777777778 Time  0.09 s\nEpoch :  17  Batch :  1850  Loss :   0.19112095058769793  Accuracy :  0.9139527027027027 Time  0.09 s\nAccuracy of     0 : 59 %\nAccuracy of     1 : 65 %\nAccuracy of     2 : 94 %\nAccuracy of     3 : 98 %\nAccuracy of     4 : 99 %\nAccuracy of     5 : 98 %\nAccuracy of     6 : 97 %\nAccuracy of     7 : 98 %\nAccuracy of     8 : 98 %\nAccuracy of     9 : 95 %\nAccuracy of    10 : 98 %\nAccuracy of    11 : 98 %\nAccuracy of    12 : 90 %\nAccuracy of    13 : 96 %\nAccuracy of    14 : 98 %\nAccuracy of    15 : 98 %\nAccuracy of    16 : 99 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 96 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 99 %\nAccuracy of    23 : 98 %\nAccuracy of    24 : 49 %\nAccuracy of    25 : 89 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 76 %\nAccuracy of    29 : 100 %\nAccuracy of    30 : 97 %\nAccuracy of    31 : 66 %\nAccuracy of    32 : 97 %\nAccuracy of    33 : 84 %\nAccuracy of    34 : 98 %\nAccuracy of    35 : 89 %\nAccuracy of    36 : 98 %\nAccuracy of    37 : 91 %\nAccuracy of    38 : 94 %\nAccuracy of    39 : 92 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 98 %\nAccuracy of    42 : 97 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 99 %\nAccuracy of    46 : 97 %\nAccuracy of    47 : 63 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 99 %\nAccuracy of    50 : 46 %\nAccuracy of    51 : 85 %\nAccuracy of    52 : 97 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 71 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 99 %\nAccuracy of    57 : 69 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 80 %\nAccuracy of    60 : 98 %\nAccuracy of    61 : 79 %\n[17 epoch] Accuracy of the network on the Training images: 91 %\nEpoch :  18  Batch :  50  Loss :   0.14765026330947875  Accuracy :  0.94 Time  0.09 s\nEpoch :  18  Batch :  100  Loss :   0.16412124842405318  Accuracy :  0.92875 Time  0.09 s\nEpoch :  18  Batch :  150  Loss :   0.16438214600086212  Accuracy :  0.9291666666666667 Time  0.09 s\nEpoch :  18  Batch :  200  Loss :   0.17207283556461334  Accuracy :  0.925 Time  0.09 s\nEpoch :  18  Batch :  250  Loss :   0.179846146941185  Accuracy :  0.92175 Time  0.09 s\nEpoch :  18  Batch :  300  Loss :   0.18367935945590338  Accuracy :  0.9191666666666667 Time  0.09 s\nEpoch :  18  Batch :  350  Loss :   0.18252720875399453  Accuracy :  0.9207142857142857 Time  0.09 s\nEpoch :  18  Batch :  400  Loss :   0.1807159526646137  Accuracy :  0.9215625 Time  0.09 s\nEpoch :  18  Batch :  450  Loss :   0.1844051194853253  Accuracy :  0.92 Time  0.1 s\nEpoch :  18  Batch :  500  Loss :   0.18505549669265747  Accuracy :  0.918375 Time  0.09 s\nEpoch :  18  Batch :  550  Loss :   0.18154816776514054  Accuracy :  0.920340909090909 Time  0.09 s\nEpoch :  18  Batch :  600  Loss :   0.18303119468192258  Accuracy :  0.9197916666666667 Time  0.11 s\nEpoch :  18  Batch :  650  Loss :   0.18423425254913478  Accuracy :  0.9188461538461539 Time  0.09 s\nEpoch :  18  Batch :  700  Loss :   0.18626339629292488  Accuracy :  0.9175892857142857 Time  0.09 s\nEpoch :  18  Batch :  750  Loss :   0.1856587383945783  Accuracy :  0.9180833333333334 Time  0.09 s\nEpoch :  18  Batch :  800  Loss :   0.1843365325592458  Accuracy :  0.9190625 Time  0.1 s\nEpoch :  18  Batch :  850  Loss :   0.18575155081117856  Accuracy :  0.919264705882353 Time  0.09 s\nEpoch :  18  Batch :  900  Loss :   0.18401699855923653  Accuracy :  0.9197222222222222 Time  0.09 s\nEpoch :  18  Batch :  950  Loss :   0.18443017245907534  Accuracy :  0.919671052631579 Time  0.09 s\nEpoch :  18  Batch :  1000  Loss :   0.1846611745506525  Accuracy :  0.91925 Time  0.09 s\nEpoch :  18  Batch :  1050  Loss :   0.18490034357422874  Accuracy :  0.9185714285714286 Time  0.09 s\nEpoch :  18  Batch :  1100  Loss :   0.1851843341507695  Accuracy :  0.9180681818181818 Time  0.09 s\nEpoch :  18  Batch :  1150  Loss :   0.18755697458982468  Accuracy :  0.9173913043478261 Time  0.09 s\nEpoch :  18  Batch :  1200  Loss :   0.18925002466887236  Accuracy :  0.9165104166666667 Time  0.09 s\nEpoch :  18  Batch :  1250  Loss :   0.18823044992685317  Accuracy :  0.91695 Time  0.09 s\nEpoch :  18  Batch :  1300  Loss :   0.1882400387640183  Accuracy :  0.9166826923076923 Time  0.11 s\nEpoch :  18  Batch :  1350  Loss :   0.18846600610900807  Accuracy :  0.9158333333333334 Time  0.09 s\nEpoch :  18  Batch :  1400  Loss :   0.18836984024516173  Accuracy :  0.9160714285714285 Time  0.09 s\nEpoch :  18  Batch :  1450  Loss :   0.18856912151492874  Accuracy :  0.9159051724137931 Time  0.09 s\nEpoch :  18  Batch :  1500  Loss :   0.1877714807887872  Accuracy :  0.91625 Time  0.09 s\nEpoch :  18  Batch :  1550  Loss :   0.1864142593356871  Accuracy :  0.9168145161290323 Time  0.09 s\nEpoch :  18  Batch :  1600  Loss :   0.1856707273889333  Accuracy :  0.9169140625 Time  0.09 s\nEpoch :  18  Batch :  1650  Loss :   0.18563902095411763  Accuracy :  0.9171969696969697 Time  0.09 s\nEpoch :  18  Batch :  1700  Loss :   0.18628218078437975  Accuracy :  0.9169485294117647 Time  0.09 s\nEpoch :  18  Batch :  1750  Loss :   0.18631502923795154  Accuracy :  0.917 Time  0.09 s\nEpoch :  18  Batch :  1800  Loss :   0.18684732457002004  Accuracy :  0.9166319444444444 Time  0.09 s\nEpoch :  18  Batch :  1850  Loss :   0.18655832545177356  Accuracy :  0.9165878378378378 Time  0.09 s\nAccuracy of     0 : 63 %\nAccuracy of     1 : 67 %\nAccuracy of     2 : 94 %\nAccuracy of     3 : 99 %\nAccuracy of     4 : 98 %\nAccuracy of     5 : 99 %\nAccuracy of     6 : 97 %\nAccuracy of     7 : 99 %\nAccuracy of     8 : 99 %\nAccuracy of     9 : 96 %\nAccuracy of    10 : 99 %\nAccuracy of    11 : 98 %\nAccuracy of    12 : 91 %\nAccuracy of    13 : 97 %\nAccuracy of    14 : 99 %\nAccuracy of    15 : 99 %\nAccuracy of    16 : 99 %\nAccuracy of    17 : 98 %\nAccuracy of    18 : 99 %\nAccuracy of    19 : 99 %\nAccuracy of    20 : 96 %\nAccuracy of    21 : 99 %\nAccuracy of    22 : 98 %\nAccuracy of    23 : 99 %\nAccuracy of    24 : 51 %\nAccuracy of    25 : 88 %\nAccuracy of    26 : 99 %\nAccuracy of    27 : 99 %\nAccuracy of    28 : 75 %\nAccuracy of    29 : 99 %\nAccuracy of    30 : 98 %\nAccuracy of    31 : 68 %\nAccuracy of    32 : 98 %\nAccuracy of    33 : 84 %\nAccuracy of    34 : 99 %\nAccuracy of    35 : 86 %\nAccuracy of    36 : 97 %\nAccuracy of    37 : 89 %\nAccuracy of    38 : 95 %\nAccuracy of    39 : 91 %\nAccuracy of    40 : 99 %\nAccuracy of    41 : 98 %\nAccuracy of    42 : 98 %\nAccuracy of    43 : 99 %\nAccuracy of    44 : 98 %\nAccuracy of    45 : 99 %\nAccuracy of    46 : 97 %\nAccuracy of    47 : 63 %\nAccuracy of    48 : 98 %\nAccuracy of    49 : 98 %\nAccuracy of    50 : 44 %\nAccuracy of    51 : 88 %\nAccuracy of    52 : 98 %\nAccuracy of    53 : 98 %\nAccuracy of    54 : 71 %\nAccuracy of    55 : 99 %\nAccuracy of    56 : 98 %\nAccuracy of    57 : 68 %\nAccuracy of    58 : 98 %\nAccuracy of    59 : 82 %\nAccuracy of    60 : 98 %\nAccuracy of    61 : 77 %\n[18 epoch] Accuracy of the network on the Training images: 91 %\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    correct=0\n",
    "    total=0\n",
    "    class_correct = list(0. for _ in classes)\n",
    "    class_total = list(0. for _ in classes)\n",
    "    for i, data in enumerate(training_generator, 0):\n",
    "        inputs, labels = data\n",
    "        t0 = time()\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        labels = eye[labels]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        _, labels = torch.max(labels, 1)\n",
    "        c = (predicted == labels.data).squeeze()\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "        accuracy = float(correct) / float(total)\n",
    "        \n",
    "        history_accuracy.append(accuracy)\n",
    "        history_loss.append(loss)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for j in range(labels.size(0)):\n",
    "            label = labels[j]\n",
    "            class_correct[label] += c[j].item()\n",
    "            class_total[label] += 1\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if(i%50==49):\n",
    "            print( \"Epoch : \",epoch+1,\" Batch : \", i+1,\" Loss :  \",running_loss/(i+1),\" Accuracy : \",accuracy,\"Time \",round(time()-t0, 2),\"s\" )\n",
    "    for k in range(len(classes)):\n",
    "        if(class_total[k]!=0):\n",
    "            print('Accuracy of %5s : %2d %%' % (classes[k], 100 * class_correct[k] / class_total[k]))\n",
    "        \n",
    "    print('[%d epoch] Accuracy of the network on the Training images: %d %%' % (epoch+1, 100 * correct / total))\n",
    "        \n",
    "    torch.save(model.state_dict(), os.path.join(PATH_SAVE,str(epoch+1)+'.pth'))\n",
    "        \n",
    "torch.save(model.state_dict(), os.path.join(PATH_SAVE,'FINAL_EPOCH'+'.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing The Training Accuracy and losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f7dad9dc2e8>]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history_accuracy)\n",
    "plt.plot(history_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('/kaggle/working/Weights/FINAL_EPOCH.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EfficientNet(\n  (_conv_stem): Conv2dStaticSamePadding(\n    3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n    (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n  )\n  (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_blocks): ModuleList(\n    (0): MBConvBlock(\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        32, 8, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        8, 32, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (1): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        96, 4, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        4, 96, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (2): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (3): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        144, 6, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        6, 144, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (4): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (5): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n        (static_padding): ZeroPad2d(padding=(0, 1, 0, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        240, 10, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        10, 240, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (6): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (7): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (8): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        480, 20, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        20, 480, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (9): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (10): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (11): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 2, 1, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        672, 28, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        28, 672, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (12): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (13): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (14): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(2, 2, 2, 2), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n    (15): MBConvBlock(\n      (_expand_conv): Conv2dStaticSamePadding(\n        192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_depthwise_conv): Conv2dStaticSamePadding(\n        1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n        (static_padding): ZeroPad2d(padding=(1, 1, 1, 1), value=0.0)\n      )\n      (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_se_reduce): Conv2dStaticSamePadding(\n        1152, 48, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_se_expand): Conv2dStaticSamePadding(\n        48, 1152, kernel_size=(1, 1), stride=(1, 1)\n        (static_padding): Identity()\n      )\n      (_project_conv): Conv2dStaticSamePadding(\n        1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n        (static_padding): Identity()\n      )\n      (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n      (_swish): MemoryEfficientSwish()\n    )\n  )\n  (_conv_head): Conv2dStaticSamePadding(\n    320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n    (static_padding): Identity()\n  )\n  (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n  (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n  (_dropout): Dropout(p=0.2, inplace=False)\n  (_fc): Linear(in_features=1280, out_features=62, bias=True)\n  (_swish): MemoryEfficientSwish()\n)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_transforms = transforms.Compose([transforms.Resize(512),\n",
    "                                      transforms.ToTensor(),\n",
    "                                     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def predict_image(image):\n",
    "    image_tensor = test_transforms(image)\n",
    "    image_tensor = image_tensor.unsqueeze_(0)\n",
    "    input = Variable(image_tensor)\n",
    "    input = input.to(device)\n",
    "    output = model(input)\n",
    "    index = output.data.cpu().numpy().argmax()\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission=pd.read_csv(BASE_PATH+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>filename</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>train_31_30009</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>train_31_29999</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>train_31_30029</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>train_31_30031</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>train_31_30019</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
      ],
      "text/plain": [
       "   Unnamed: 0        filename  class\n0           0  train_31_30009      0\n1           1  train_31_29999      0\n2           2  train_31_30029      0\n3           3  train_31_30031      0\n4           4  train_31_30019      0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ ================================ >]  0.03  % Complete\n[ ================================ >]  3.26  % Complete\n[ ================================ >]  6.48  % Complete\n[ ================================ >]  9.71  % Complete\n[ ================================ >]  12.94  % Complete\n[ ================================ >]  16.16  % Complete\n[ ================================ >]  19.39  % Complete\n[ ================================ >]  22.61  % Complete\n[ ================================ >]  25.84  % Complete\n[ ================================ >]  29.06  % Complete\n[ ================================ >]  32.29  % Complete\n[ ================================ >]  35.52  % Complete\n[ ================================ >]  38.74  % Complete\n[ ================================ >]  41.97  % Complete\n[ ================================ >]  45.19  % Complete\n[ ================================ >]  48.42  % Complete\n[ ================================ >]  51.65  % Complete\n[ ================================ >]  54.87  % Complete\n[ ================================ >]  58.1  % Complete\n[ ================================ >]  61.32  % Complete\n[ ================================ >]  64.55  % Complete\n[ ================================ >]  67.77  % Complete\n[ ================================ >]  71.0  % Complete\n[ ================================ >]  74.23  % Complete\n[ ================================ >]  77.45  % Complete\n[ ================================ >]  80.68  % Complete\n[ ================================ >]  83.9  % Complete\n[ ================================ >]  87.13  % Complete\n[ ================================ >]  90.35  % Complete\n[ ================================ >]  93.58  % Complete\n[ ================================ >]  96.81  % Complete\n[ ================================ >]  100.0  % Complete\n"
     ]
    }
   ],
   "source": [
    "IMG_TEST_PATH=os.path.join(BASE_PATH,'test_images/')\n",
    "for i in range(len(submission)):\n",
    "    img=Image.open(IMG_TEST_PATH+submission.iloc[i][1]+'.png')\n",
    "    submission['class'][i]=predict_image(img)\n",
    "    if(i%100==0 or i==len(submission)-1):\n",
    "        print('[',32*'=','>] ',round((i+1)*100/len(submission),2),' % Complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "version": "3.6.4",
   "file_extension": ".py",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "name": "python",
   "mimetype": "text/x-python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
